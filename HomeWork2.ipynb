{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#variable declaration\n",
    "\n",
    "fnames=['video1(00h00m27s-00h01m01s).feat','video2(00h00m37s-00h01m24s).feat','video3(00h00m18s-00h00m49s).feat','video4(00h00m12s-00h00m48s).feat', 'video5(00h01m08s-00h01m37s).feat', 'video6(00h00m25s-00h01m06s).feat', 'video7(00h00m11s-00h00m45s).feat', 'video8(00h05m29s-00h05m58s).feat', 'video9(00h03m52s-00h04m31s).feat', 'video10(00h00m29s-00h00m58s).feat', 'video11(00h02m38s-00h03m15s).feat', 'video12(00h00m39s-00h01m14s).feat', 'video13(00h00m13s-00h00m44s).feat', 'video14(00h00m00s-00h00m29s).feat', 'video15(00h00m21s-00h00m56s).feat', 'video16(00h00m20s-00h00m50s).feat', 'video17(00h02m09s-00h02m28s).feat', 'video18(00h00m33s-00h01m04s).feat', 'video19(00h00m42s-00h01m12s).feat', 'video20(00h00m51s-00h01m11s).feat', 'video21(00h00m11s-00h00m43s).feat', 'video22(00h00m34s-00h01m10s).feat', 'video23(00h00m15s-00h00m54s).feat', 'video24(00h00m01s-00h00m35s).feat', 'video25(00h00m15s-00h00m46s).feat', 'video26(00h00m48s-00h01m24s).feat', 'video27(00h01m16s-00h01m47s).feat', 'video28(00h00m03s-00h00m36s).feat', 'video29(00h00m00s-00h00m29s).feat', 'video30(00h02m01s-00h02m33s).feat', 'video31(00h00m06s-00h00m42s).feat', 'video32(00h00m15s-00h00m45s).feat', 'video33(00h00m17s-00h01m15s).feat', 'video34(00h00m05s-00h00m33s).feat', 'video35(00h00m19s-00h00m56s).feat', 'video36(00h00m09s-00h00m46s).feat', 'video37(00h01m24s-00h01m58s).feat', 'video38(00h00m23s-00h00m57s).feat', 'video39(00h00m38s-00h01m09s).feat', 'video40(00h00m12s-00h00m38s).feat', 'video41(00h00m01s-00h00m27s).feat', 'video42(00h00m35s-00h01m22s).feat', 'video43(00h00m15s-00h00m52s).feat', 'video44(00h00m11s-00h00m40s).feat', 'video45(00h00m06s-00h00m37s).feat', 'video46(00h00m27s-00h01m06s).feat', 'video47(00h00m18s-00h00m53s).feat', 'video48(00h00m12s-00h00m46s).feat',\n",
    "]\n",
    "\n",
    "#stopterms\n",
    "stop_words_list=[[1,777],[1,395], [1,894], [1,458], [1,726], [1,250], [2,291], [2,741], [2,160], [2,194], [2,539], [2,191], [2,122], [2,1138], [2,472], [2,457], [2,449], [3,250], [3,258], [3,976], [3,1327], [3,303], [4,96], [4,206], [4,440], [4,813], [4,249], [4,332], [4,656], [4,778], [5,264], [5,640], [5,1059], [5,830], [6,358], [6,714], [6,1195], [6,282], [6,1648], [7,294], [7,203], [7,539], [7,418], [7,790], [7,338], [7,898], [8,577], [8,1422], [8,536], [8,466], [9,1385], [9,681], [9,1946], [10,388], [10,419], [10,132], [10,462], [10,158], [10,123], [10,510], [10,568], [10,43], [11,462], [11,273], [11,864], [11,1790], [11,311], [12,134], [12,300], [12,254], [12,541], [12,163], [12,272], [12,508], [12,314], [12,479], [12,800], [13,214], [13,363], [13,773], [13,504], [13,115], [13,117], [13,552], [13,480], [14,39], [14,299], [14,723], [14,719], [14,375], [14,198], [14,330], [15,122], [15,378], [15,260], [15,207], [15,367], [15,1254], [15,901], [16,293], [16,860], [16,206], [16,329], [16,1226], [17,663], [17,148], [17,127], [17,515], [17,416], [18,205], [18,1002], [18,877], [18,423], [18,512], [18,375], [19,766], [19,254], [19,165], [19,217], [19,854], [19,479], [19,321], [21,812], [21,221], [21,424], [21,431], [21,735], [21,514], [22,535], [22,966], [22,600], [22,1398], [23,209], [23,320], [23,558], [23,638], [23,630], [23,747], [23,669], [23,360], [24,484], [24,343], [24,591], [24,314], [24,59], [24,162], [24,191], [24,605], [24,704], [25,312], [25,192], [25,1013], [25,273], [25,446], [25,506], [25,342], [26,863], [26,563], [26,610], [26,1465], [27,371], [27,1417], [27,953], [27,605], [28,174], [28,203], [28,222], [28,371], [28,272], [28,360], [28,443], [28,591], [28,267], [28,401], [29,205], [29,144], [29,531], [29,727], [29,598], [29,621], [30,340], [30,1007], [30,1024], [30,847], [31,752], [31,790], [31,585], [31,426], [31,1134], [32,428], [32,393], [32,227], [32,761], [32,514], [32,694], [33,1093], [33,903], [33,1656], [33,1936], [33,323], [34,499], [34,454], [34,450], [34,513], [34,896], [35,336], [35,995], [35,970], [35,363], [35,441], [35,765], [36,265], [36,488], [36,140], [36,200], [36,172], [36,885], [36,366], [36,629], [36,634], [37,584], [37,558], [37,1088], [37,454], [37,713], [38,250], [38,717], [38,365], [38,183], [38,1530], [38,389], [39,296], [39,529], [39,201], [39,566], [39,903], [39,692], [40,608], [40,934], [40,580], [40,441], [41,505], [41,682], [41,1037], [41,415], [42,486], [42,155], [42,665], [42,438], [42,309], [42,760], [42,310], [42,766], [42,903], [43,380], [43,782], [43,430], [43,772], [43,423], [43,1000], [44,436], [44,899], [44,421], [44,1190], [45,403], [45,508], [45,900], [45,837], [45,449], [46,115], [46,3056], [46,785], [47,1101], [47,1555], [47,642], [47,287], [48,280], [48,1013], [48,1190], [48,921],\n",
    "]\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.189522552586 0.14819 0.0 0.6698 0.148963457134 21 812\n",
      "0.174080246606 0.13956 0.0019614 0.66697 0.142627464721 21 221\n",
      "0.169468687005 0.144645 0.00058798 0.64659 0.134052020356 21 424\n",
      "0.151174594292 0.12367 0.00030469 0.60998 0.119555723555 21 431\n",
      "0.141749349415 0.12525 0.00049013 0.56181 0.103465087653 21 735\n",
      "0.152197621868 0.13806 0.00031486 0.60773 0.106086211575 21 514\n",
      "0.130226155479 0.080246 0.0 0.73494 0.141760811682 22 535\n",
      "0.0919613740849 0.0587255 5.5266e-05 0.57407 0.0999250819419 22 966\n",
      "0.0832872414333 0.0625365 0.00015271 0.49571 0.0811233481864 22 600\n",
      "0.106641198398 0.078488 0.00019553 0.60293 0.0987570056746 22 1398\n",
      "0.106421756603 0.084743 0.0 0.50109 0.100793441865 23 209\n",
      "0.0910276049031 0.0428705 6.441e-05 0.50715 0.109202946705 23 320\n",
      "0.113329861971 0.0689055 0.00033262 0.55138 0.116375284239 23 558\n",
      "0.119419193119 0.0877365 0.00012076 0.56946 0.114499469574 23 638\n",
      "0.101840957325 0.0660925 1.5436e-05 0.55768 0.105306347626 23 630\n",
      "0.102127915114 0.064869 0.00029789 0.59147 0.106121707217 23 747\n",
      "0.112970354294 0.074975 7.1923e-05 0.49545 0.107927930307 23 669\n",
      "0.0984081348611 0.0593685 0.00021146 0.49777 0.105068103695 23 360\n",
      "0.122024282748 0.094243 0.0 0.52296 0.107067766126 24 484\n",
      "0.0877837284869 0.05981 9.5301e-05 0.58983 0.0940389856811 24 343\n",
      "0.099049920846 0.070888 0.00014535 0.50283 0.0929657067681 24 591\n",
      "0.102436591854 0.0678335 6.5704e-05 0.47999 0.0983299701355 24 314\n",
      "0.0634651161017 0.056366 0.0004477 0.37627 0.0625962356724 24 59\n",
      "0.0674760087037 0.038017 0.00011731 0.33293 0.0721605526691 24 162\n",
      "0.108813515131 0.075298 0.0002171 0.58225 0.108076252625 24 191\n",
      "0.116745081893 0.080804 4.8781e-05 0.48374 0.110172949571 24 605\n",
      "0.111191047459 0.0805955 5.8196e-05 0.52183 0.10324907487 24 704\n",
      "0.0692351523397 0.036148 0.0 0.38758 0.0802260321841 25 312\n",
      "0.0522320497917 0.0427435 0.00081593 0.20251 0.041793788846 25 192\n",
      "0.220401285439 0.17968 0.00062495 0.67258 0.180211894076 25 1013\n",
      "0.164891187106 0.1027 0.00047255 0.63225 0.160781173376 25 273\n",
      "0.074730275 0.0438925 0.00020908 0.56749 0.0878762840716 25 446\n",
      "0.17640680498 0.116595 0.0002104 0.70001 0.172044660263 25 506\n",
      "0.111000591316 0.064635 0.00022891 0.64268 0.124449851643 25 342\n",
      "0.104585486315 0.078413 0.0 0.59139 0.100375594465 26 863\n",
      "0.13987053389 0.097154 0.00027115 0.7315 0.134720812085 26 563\n",
      "0.12435686082 0.073332 0.0010169 0.65305 0.128655047805 26 610\n",
      "0.149984246027 0.11417 0.00048294 0.73727 0.127989271983 26 1465\n",
      "0.154801726846 0.088431 0.0 0.66469 0.162909024356 27 371\n",
      "0.126719259139 0.088143 0.00049784 0.68282 0.124557489554 27 1417\n",
      "0.140234650829 0.1161 0.00018144 0.56533 0.110763045021 27 953\n",
      "0.100146775273 0.067549 0.00032515 0.56487 0.0997848979464 27 605\n",
      "0.0859944235632 0.070401 0.0 0.38657 0.0731554885719 28 174\n",
      "0.0939576880788 0.074694 0.00074548 0.55685 0.0758069183422 28 203\n",
      "0.0994282355856 0.0848255 0.0022723 0.53652 0.0785475234735 28 222\n",
      "0.0753117195418 0.058896 0.00075214 0.34398 0.0630621083838 28 371\n",
      "0.149353865074 0.101805 0.0026012 0.63589 0.139803599829 28 272\n",
      "0.123359615972 0.106105 0.00040289 0.56599 0.0978655021859 28 360\n",
      "0.0984110408578 0.083651 0.0013574 0.37964 0.0677152673916 28 443\n",
      "0.0814120469374 0.066134 0.00012247 0.44098 0.071039051925 28 591\n",
      "0.0914629444944 0.078651 0.00037127 0.30935 0.0638374255989 28 267\n",
      "0.0994986912718 0.08088 0.0020079 0.3796 0.075484522432 28 401\n",
      "0.106029665366 0.077857 0.0 0.51222 0.100572405159 29 205\n",
      "0.0960512194444 0.0783205 0.0014786 0.34113 0.0741317464524 29 144\n",
      "0.115975980056 0.095469 0.00076021 0.56812 0.086491905588 29 531\n",
      "0.110225574003 0.085137 0.0020865 0.51133 0.0886558078423 29 727\n",
      "0.125954498562 0.106405 0.00090482 0.48492 0.0919535713234 29 598\n",
      "0.110939797617 0.092887 0.0005257 0.45653 0.0849457373265 29 621\n",
      "0.0628943171471 0.045771 0.0 0.29419 0.0618107935735 30 340\n",
      "0.0971826085005 0.074615 0.00010049 0.6977 0.0864025491632 30 1007\n",
      "0.0985312303613 0.062938 0.00016998 0.57065 0.103424325901 30 1024\n",
      "0.0674885087957 0.048064 0.0001097 0.48871 0.0677011929139 30 847\n",
      "0.0624331889096 0.0407375 0.0 0.42257 0.068187432379 31 752\n",
      "0.0716668975696 0.046361 9.266e-05 0.53894 0.0785440224448 31 790\n",
      "0.0715371846154 0.05045 0.00019722 0.42649 0.0720182887686 31 585\n",
      "0.0570339770939 0.0341015 7.0442e-05 0.56708 0.0727542959294 31 426\n",
      "0.0485025242028 0.025357 9.0888e-05 0.39099 0.0607509777236 31 1134\n",
      "0.154143504112 0.110965 0.0 0.66617 0.147579293142 32 428\n",
      "0.137013313181 0.097317 0.00024236 0.57459 0.130947602564 32 393\n",
      "0.140371660529 0.11489 0.00085924 0.59228 0.11685433208 32 227\n",
      "0.146880565414 0.12038 0.00037596 0.68708 0.125527840392 32 761\n",
      "0.1422075807 0.11329 0.00050485 0.60543 0.121110306419 32 514\n",
      "0.178279659841 0.133185 0.00072306 0.73554 0.163539721497 32 694\n",
      "0.176801808692 0.11176 0.0 0.76397 0.16963256404 33 1093\n",
      "0.172697896722 0.12403 0.00021273 0.81187 0.160047281875 33 903\n",
      "0.182132871721 0.119215 0.00019226 0.79687 0.178844491448 33 1656\n",
      "0.162648624406 0.092942 0.00027949 0.79062 0.172100969881 33 1936\n",
      "0.27062265387 0.25769 0.003318 0.79829 0.200842022098 33 323\n",
      "0.0854216781764 0.046184 0.0 0.59788 0.0997204600828 34 499\n",
      "0.0847042679295 0.0609715 0.00019566 0.48285 0.0867256703311 34 454\n",
      "0.104635056689 0.076304 0.00028327 0.44665 0.0918491453865 34 450\n",
      "0.0850847692788 0.063721 0.0001169 0.64218 0.0848397108592 34 513\n",
      "0.0832550886429 0.0631465 6.6718e-05 0.56136 0.0791217184056 34 896\n",
      "0.0889564325893 0.0572815 0.0 0.48118 0.0930478494762 35 336\n",
      "0.0998135798995 0.072724 0.00031244 0.52408 0.0965883535259 35 995\n",
      "0.100697505495 0.0778685 0.0001758 0.45581 0.0839703922736 35 970\n",
      "0.118096468595 0.086618 0.0019633 0.63583 0.109851424038 35 363\n",
      "0.104362355351 0.071448 0.00072315 0.55638 0.100548006372 35 441\n",
      "0.101341646379 0.082892 0.00023519 0.49221 0.0871876892479 35 765\n",
      "0.0848312388302 0.055816 0.0 0.48883 0.0896889274069 36 265\n",
      "0.0804145171926 0.0503045 0.0002469 0.51808 0.0844852992006 36 488\n",
      "0.140116206929 0.089038 0.00047973 0.55048 0.138836225014 36 140\n",
      "0.098606013 0.0554035 0.0017688 0.54061 0.114912990577 36 200\n",
      "0.11530169186 0.061029 0.00017784 0.52192 0.125809562404 36 172\n",
      "0.109552153955 0.077014 0.00016125 0.56875 0.110934686195 36 885\n",
      "0.192124793443 0.11934 0.00020981 0.66404 0.178646535402 36 366\n",
      "0.117123276948 0.07899 0.0010724 0.7121 0.119091094239 36 629\n",
      "0.0996150103312 0.0680985 0.00057391 0.48517 0.0940911675369 36 634\n",
      "0.192670409589 0.15923 0.0 0.68792 0.138713602409 37 584\n",
      "0.191217930287 0.166695 0.0025169 0.61068 0.139191168799 37 558\n",
      "0.22193424614 0.19836 0.0017955 0.67922 0.151326704911 37 1088\n",
      "0.154896070485 0.12522 0.0030963 0.60177 0.118783093265 37 454\n",
      "0.200026591725 0.16801 0.001018 0.63078 0.151361792888 37 713\n",
      "0.1181067276 0.0975665 0.0 0.45003 0.102726621435 38 250\n",
      "0.10411829749 0.080271 0.0013393 0.52996 0.0913467258808 38 717\n",
      "0.133482976712 0.099971 0.0010201 0.53388 0.115270869059 38 365\n",
      "0.0925538 0.077647 0.0021763 0.34526 0.0736929869425 38 183\n",
      "0.114510300967 0.0882675 0.00095978 0.64582 0.0985189298789 38 1530\n",
      "0.11104867054 0.075179 0.00030363 0.51888 0.102126184117 38 389\n",
      "0.150808275676 0.108855 0.0 0.56922 0.131340721972 39 296\n",
      "0.149921233459 0.11718 0.0034157 0.62385 0.120155913575 39 529\n",
      "0.176770134826 0.15335 0.0094028 0.57506 0.116379738139 39 201\n",
      "0.133441145053 0.105215 0.0026652 0.60814 0.103096928319 39 566\n",
      "0.154052788151 0.12683 0.0021114 0.84491 0.116869100657 39 903\n",
      "0.155459124855 0.12959 0.0022591 0.73445 0.120485616778 39 692\n",
      "0.0614228286842 0.040266 0.0 0.44914 0.0653706091128 40 608\n",
      "0.0827843545289 0.04605 0.00013631 0.59592 0.0929034609095 40 934\n",
      "0.103082566155 0.0710255 0.00014349 0.52568 0.103481831275 40 580\n",
      "0.0819269942358 0.060784 8.4735e-05 0.48262 0.080573794588 40 441\n",
      "0.0644609772079 0.031655 0.0 0.46462 0.080707687458 41 505\n",
      "0.0451084160699 0.0192085 4.0322e-06 0.51508 0.0626296288622 41 682\n",
      "0.107375306461 0.050268 5.775e-06 0.71592 0.133911597304 41 1037\n",
      "0.0675771721735 0.0316 1.8559e-05 0.55171 0.107178825087 41 415\n",
      "0.0923145341564 0.0676265 0.0 0.48925 0.0848240859076 42 486\n",
      "0.132245925161 0.089713 0.0021164 0.56842 0.123823618793 42 155\n",
      "0.117643581474 0.081326 0.00026767 0.6148 0.115502093094 42 665\n",
      "0.103309436986 0.078581 0.0030234 0.46464 0.0904137936329 42 438\n",
      "0.110445213916 0.086822 0.0019201 0.42064 0.0887815093422 42 309\n",
      "0.163728629342 0.11796 0.0012672 0.77052 0.151953855741 42 760\n",
      "0.188542142258 0.14544 0.00045218 0.75484 0.156335480046 42 310\n",
      "0.109476183995 0.0777415 0.00042386 0.66015 0.106788817764 42 766\n",
      "0.144459636213 0.10653 0.0012064 0.71393 0.12961309509 42 903\n",
      "0.0682350767895 0.053636 0.0 0.45324 0.0662470186082 43 380\n",
      "0.094462509156 0.067142 0.00028491 0.6312 0.0946694091264 43 782\n",
      "0.117958618488 0.0759065 0.00031567 0.52442 0.11940511309 43 430\n",
      "0.0848679106347 0.0585385 0.00023901 0.4657 0.0910270277351 43 772\n",
      "0.122018367683 0.083255 0.00058834 0.55953 0.124414730139 43 423\n",
      "0.11970106776 0.0722325 0.00024857 0.61257 0.131760452718 43 1000\n",
      "0.0614404045872 0.0463135 0.0 0.60909 0.0632919637653 44 436\n",
      "0.105567094727 0.068561 0.00025043 0.72491 0.105802487541 44 899\n",
      "0.0667787650594 0.048672 0.00057233 0.51964 0.0652919739797 44 421\n",
      "0.0890910070336 0.056856 0.00015705 0.59663 0.0982169278893 44 1190\n",
      "0.0714878983623 0.041899 0.0 0.37664 0.0810031187817 45 403\n",
      "0.0836066562165 0.0417455 8.6251e-05 0.59657 0.104429597729 45 508\n",
      "0.0764351333544 0.0379155 1.1317e-05 0.69106 0.098192806008 45 900\n",
      "0.104546742639 0.062895 2.8076e-05 0.54519 0.113139912464 45 837\n",
      "0.0848988086748 0.029563 2.0428e-05 0.51832 0.115430083145 45 449\n",
      "0.117564271304 0.096256 0.0 0.43471 0.104199305321 46 115\n",
      "0.145137171299 0.11017 0.0001067 0.62586 0.129392032973 46 3056\n",
      "0.154261071962 0.11738 0.00030103 0.76619 0.132627663993 46 785\n",
      "0.194238738992 0.16938 0.0 0.70015 0.145320527344 47 1101\n",
      "0.184288130141 0.14178 0.00074047 0.77089 0.150411096768 47 1555\n",
      "0.195760799377 0.187275 0.0017935 0.64846 0.137633032367 47 642\n",
      "0.22123206561 0.19603 0.00067963 0.6367 0.137756885763 47 287\n",
      "0.184225648929 0.15066 0.0 0.66463 0.147190768691 48 280\n",
      "0.0998983354107 0.068134 9.7351e-05 0.57926 0.097571435376 48 1013\n",
      "0.101027133999 0.0636655 1.4699e-05 0.62582 0.112865291687 48 1190\n",
      "0.0742431181107 0.042277 1.8637e-05 0.53268 0.0873454433633 48 921\n"
     ]
    }
   ],
   "source": [
    "#Getting the audio and video features: mean, median, max, min and standard deviation\n",
    "\n",
    "file_number=21\n",
    "for findex in fnames[20:]:\n",
    "    f=\"/Users/apple/Desktop/USC/*Sem 3/CSCI 535/Homework/Homework_1/features/AcousticFeatures/\"+findex\n",
    "    fo=open(f,\"r+\")\n",
    "    str=fo.read();\n",
    "    mylist = str.split('\\n')\n",
    "\n",
    "    acoustic_features=[] #holds the input acoustic file as a list\n",
    "    \n",
    "    for i in mylist:\n",
    "        acoustic_features.append(i.split(','))\n",
    "        \n",
    "    stop_points=[]\n",
    "    #print str,i\n",
    "    for i in stop_words_list:\n",
    "        if i[0]==file_number:\n",
    "            stop_points.append(i[1])\n",
    "    \n",
    "    start=0\n",
    "    end=stop_points[0]\n",
    "    i=0\n",
    "    while i<len(stop_points):#Summarising for segment\n",
    "        feature=[]\n",
    "        for j in acoustic_features[start:end]:\n",
    "            f=j[6]        #change the FEATURE HERE\n",
    "            total_break=0\n",
    "            if f==[''] or f==\" \" or f.lower()=='-inf'or f.lower()=='inf' or f.lower()=='nan':\n",
    "                total_break+=1\n",
    "            else:    \n",
    "                feature.append(float(f))\n",
    "        \n",
    "        print np.mean(feature),np.median(feature),min(feature),max(feature),np.std(feature),file_number,end-start\n",
    "\n",
    "        start=end+1\n",
    "        if i+1<len(stop_points):\n",
    "            end=stop_points[i+1]+start\n",
    "        i+=1\n",
    "        \n",
    "    file_number+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Defining the features data\n",
    "\n",
    "#naq_mean=[0.233433266,0.271499846,0.194656705,0.181773902,0.241251601,0.265086984,0.14201744,0.150883289,0.09933675,0.177312719,0.145586967,0.157967267,0.102980098,0.158541599,0.133451922,0.169483455,0.162872673,0.065124556,0.040800376,0.095198734,0.111916183,0.094448743,0.167350417,0.14658334,0.077898064,0.133047281,0.130726566,0.129402373,0.190965075,0.134988408,0.210779379,0.284076145,0.282775033,0.279064789,0.088195386,0.075816688,0.085474097,0.089118429,0.09717783,0.196630779,0.136723469,0.173573746,0.143170418,0.155082261,0.197227479,0.18364364,0.17065009,0.150120044,0.158545951,0.190377017,0.095941138,0.088948628,0.095032433,0.184354567,0.146712652,0.158073212,0.166655461,0.198684544,0.204016179,0.190650649,0.172986713,0.162084907,0.097096912,0.111067612,0.107732716,0.113447461,0.108421046,0.148826254,0.123528583,0.14933202,0.101471251,0.149936828,0.13737821,0.122858894,0.124237705,0.114269962,0.10661225,0.113119543,0.133753734,0.106586294,0.111790285,0.117011696,0.121608615,0.110669636,0.112977217,0.207469128,0.203602418,0.198745058,0.182456366,0.205799715,0.174582131,0.190026548,0.125836185,0.130621989,0.13953115,0.130320763,0.14078369,0.127375169,0.123354514,0.137876824,0.155466789,0.177494796,0.150001748,0.176117282,0.06978053,0.079354804,0.086266551,0.078232799,0.061032073,0.217371893,0.196512468,0.192091057,0.204160541,0.207611111,0.194708957,0.109859435,0.160428161,0.157931315,0.068432897,0.082929456,0.123617443,0.149811612,0.224884368,0.156521683,0.158232745,0.186290634,0.174714126,0.216954183,0.146484481,0.127483821,0.134983786,0.148051636,0.119505292,0.142189816,0.164576201,0.12276129,0.119109797,0.144839648,0.156252937,0.130409111,0.138118094,0.098845853,0.113236254,0.087506603,0.134124237,0.126118881,0.10164489,0.08896259,0.09026414,0.151856654,0.154371328,0.075286625,0.122685473,0.173466413,0.099856961,0.149086665,0.079312754,0.088751116,0.050849945,0.081300769,0.206741905,0.185099956,0.167679921,0.167095068,0.125106414,0.107847599,0.125440135,0.130660811,0.105050833,0.139695575,0.139769041,0.10186783,0.124587368,0.144413382,0.125683249,0.123190931,0.129270485,0.126442667,0.137105918,0.143115209,0.13987606,0.146102583,0.126892429,0.130310186,0.133237326,0.117311405,0.127334326,0.134640594,0.120555996,0.122867691,0.150556458,0.174932855,0.134372443,0.155805676,0.128707759,0.170925712,0.196988114,0.140342414,0.126459448,0.082099529,0.152395788,0.167399196,0.164478806,0.16140644,0.166261143,0.097555696,0.092599453,0.095342893,0.087218212,0.091045983,0.085570759,0.120936671,0.127678625,0.055481939,0.102123116,0.135683123,0.115429975,0.158271263,0.148361085,0.129500168,0.12228228,0.106734365,0.10085773,0.135020088,0.163556731,0.104232138,0.12925763,0.126489795,0.132754126,0.135753608,0.136219423,0.197920677,0.181166628,0.170100413,0.198505104,0.179999269,0.181711035,0.142399912,0.143500389,0.138368878,0.133054768,0.131471434,0.154548359,0.096588088,0.110295519,0.152070944,0.093506808,0.157910365,0.152843148,0.152592642,0.157803845,0.1540289,0.163234161,0.164717929,0.196798821,0.1738565,0.163018249,0.179060701,0.20017947,0.171138205,0.143255756,0.13949867,0.11273704,0.127854489,0.085560702,0.122264707,0.102576968,0.104332266,0.096112784,0.133479357,0.069399722,0.06572157,0.175185205,0.175611991,0.181281126,0.151706125,0.083383129,0.048101161,0.050596063,0.045975022]\n",
    "#energy_max=[-11.208,-18.547,-14.83,-10.516,-12.5,-17.119,-16.4,-17.655,-19.135,-17.814,-18.483,-18.038,-19.325,-18.514,-18.727,-18.227,-18.255,-5.6344,-7.9882,-6.9795,-6.8036,-6.6602,-17.798,-21.266,-18.452,-14.014,-16.339,-18.413,-14.837,-15.474,-22.777,-17.979,-20.688,-15.973,-16.314,-22.076,-17.7,-26.755,-18.701,-9.7517,-9.9687,-9.1842,-8.5776,-9.5226,-8.8492,-8.635,-23.977,-26.962,-25.326,-29.228,-19.844,-21.628,-18.628,-25.859,-26.52,-31.987,-28.134,-24.065,-28.983,-26.637,-22.581,-34.306,-22.065,-22.682,-21.561,-21.629,-22.797,-21.176,-21.359,-21.731,-20.035,-22.634,-24.275,-20.874,-19.063,-20.547,-19.878,-14.978,-17.513,-18.495,-16.962,-26.426,-15.458,-17.755,-17.737,-40.291,-13.895,-14.712,-14.27,-13.925,-18.908,-14.965,-22.965,-19.307,-19.285,-19.646,-18.683,-17.558,-18.264,-23.916,-24.178,-26.611,-23.528,-16.036,-15.709,-20.187,-21.008,-19.976,-21.744,-14.967,-14.414,-13.377,-16.635,-15.508,-16.935,-17.557,-26.573,-24.597,-21.946,-23.518,-26.081,-22.883,-16.865,-18.099,-17.734,-17.248,-18.192,-18.888,-15.93,-17.433,-14.496,-20.129,-17.896,-16.903,-17.16,-14.052,-20.001,-14.417,-12.332,-20.743,-17.395,-18.882,-15.615,-21.999,-24.13,-20.997,-17.459,-18.522,-16.455,-12.212,-24.999,-21.814,-23.062,-21.656,-23.051,-17.011,-21.823,-18.317,-21.961,-21.073,-14.77,-15.144,-15.735,-15.082,-7.7265,-10.285,-10.421,-8.9023,-11.87,-7.173,-6.1893,-9.9891,-6.2378,-12.857,-8.5443,-14.218,-6.4529,-8.7346,-8.4595,-5.8888,-12.62,-13.418,-12.659,-14.159,-10.831,-10.064,-11.243,-10.745,-11.875,-11.654,-11.301,-8.0795,-8.4027,-10.6,-8.6388,-17.34,-19.591,-17.68,-16.553,-17.583,-12.621,-17.39,-18.668,-16.049,-14.852,-12.046,-10.832,-10.495,-9.0872,-12.459,-11.972,-15.106,-14.245,-20.726,-13.028,-14.34,-14.158,-15.02,-11.979,-11.774,-21.263,-19.329,-23.175,-25.87,-23.426,-17.966,-15.747,-16.975,-20.382,-14.937,-15.616,-13.122,-11.387,-11.626,-12.587,-10.95,-10.485,-8.1992,-10,-11.287,-9.1056,-18.004,-15.411,-21.786,-19.07,-17.608,-18.047,-17.281,-16.786,-18.167,-16.718,-17.667,-17.057,-16.759,-28.802,-26.599,-19.263,-23.362,-28.275,-21.126,-12.5,-12.151,-13.167,-11.827,-18.397,-16.451,-16.908,-15.61,-18.928,-24.456,-22.871,-27.409,-24.319,-20.107,-27.523,-24.048,-26.433,-20.194,-18.782,-16.252]\n",
    "#energy_slope_max=[-0.00023906,-0.0010485,-0.00096011,-0.000249235,-0.00123325,0.000255485,-0.00084306,-0.00064263,-0.00174465,-0.00161535,-0.00079753,-0.00049242,-0.0011461,-0.00174265,-0.000739675,-0.0014926,-0.0010521,-0.00072141,-0.000548755,-0.000461685,-0.0010188,-0.00076539,-0.003864,-0.00139705,-0.00078591,-0.00109,-0.0012013,-0.0023102,-0.00117795,-0.0013958,0.000017132,-0.000286725,-0.00068579,-0.00019062,-0.00100905,-0.00117585,-0.00095941,-0.00204075,-0.00170465,-0.00056613,-0.0019903,-0.00076695,-0.0009804,-0.000726225,-0.00058993,-0.000517895,-0.001432,-0.00149725,-0.0013889,-0.00079545,-0.001398,-0.001343,-0.00116065,-0.000816985,-0.00095986,-0.001097795,-0.000617145,-5.02056E-05,-0.0010514,-0.000149605,-0.0003185,-0.001048,-0.0013354,-0.0028759,-0.00147305,-0.00168595,-0.00206,-0.00015837,-0.000904105,-0.00123775,-0.00040593,-0.00018011,-0.0011001,-0.0008004,-0.00061775,-0.0011425,-0.00043985,0.00014893,-0.0020323,-0.00049842,-0.0013289,-0.00097283,-0.00052939,-0.001082,-0.00067507,0.00061928,-0.00023691,-0.0007423,-0.0010235,-0.00073759,-0.000928245,-0.001000025,-0.000056575,-0.000605825,4.70795E-05,-0.00084854,-0.00073307,-0.00051498,-0.00076538,-0.00112,-0.00184585,-0.00134605,-0.00066146,-0.000753205,-0.0012344,-0.00020283,-0.00055849,-0.00067337,-0.000614695,-0.0011557,-0.000576925,-0.00047617,-0.00095116,-0.00052791,-0.00092257,-3.22475E-07,-5.73135E-06,-0.000037702,-0.000021273,-0.000019336,-4.3154E-06,-3.7812E-06,-0.00024645,-0.00077746,-0.00109845,-0.0028431,-0.0017232,-0.00143025,-0.0013881,-0.00131025,-0.00129055,-0.00228205,-0.001746,-0.0011588,-0.00150425,-0.0010743,-0.0006528,-0.0013272,-0.0010853,-0.000052854,-0.00129075,-0.0015457,-0.00095946,-0.000105675,-0.0016839,-0.000570375,-0.0014871,-0.00098162,-0.0007609,-0.00192975,-0.000220885,-0.0014863,-0.0024403,-0.001213,-0.0028386,-0.0014414,-0.0015378,-0.0014152,-0.0015017,-0.0014192,-0.00028111,-0.00052064,-0.00074356,-0.00069335,-0.0012527,-0.0025855,-0.0020994,-0.00094684,-0.00161895,-0.00099622,-0.0017598,-0.00094447,-0.0011068,-0.0025307,-0.00097978,-0.00297015,-0.001655,-0.0016076,-0.00188745,-0.0015515,-0.00117175,-0.0013592,-0.000785485,-0.0025872,-0.001455,-0.0012453,-0.00067105,-0.0012726,-0.00127425,-0.00162825,-0.00084351,-0.00097614,-0.000048319,-0.000130464,-0.00098546,-0.00072157,-0.00095007,-0.000535415,-0.001287,0.00024944,-0.00027088,-0.00067848,-0.00052253,-0.00047729,-0.000810265,-0.000714215,-0.0014547,-0.00120495,-0.00044116,-0.0014322,-0.0011869,-0.00037495,-0.0018881,-0.002088,-0.0024025,-0.00198495,-0.0013053,-0.00043451,-0.0014129,-0.00138075,-0.00082264,-0.0016834,-0.000869815,-0.00104835,-0.0011629,-0.0009097,-0.00070146,-0.00029846,-0.00079871,-0.00022916,-0.0004126,-0.0013814,-0.0016878,-0.00098295,-0.00110285,-0.0015368,-0.00136345,-0.00106665,-0.00120665,-0.00110835,-0.0017624,-0.00078145,-0.000993655,-0.00090133,-0.0010509,-0.000898705,-0.0024303,-0.0015732,-0.0011974,-0.0024291,-0.0011884,-0.00110795,-0.00091432,-0.0013367,-0.00030596,-0.0013265,-0.00151965,-0.00138875,-0.0016814,-0.00165515,-0.00111125,-0.0011542,-0.0014889,-0.000968205,-0.00079702,-0.00145045,-0.0019943,-0.0011395,-0.0016518,-0.001233,-0.00126725,-0.00049703,-0.00069208,-0.0010164,-0.00125275,-0.00066383,-0.00185585,-0.00089014,-0.0013696,-0.00054459]\n",
    "#gaze_up_down_min=[7,17,16,20,13,17,10,12,10,16,12,11,11,15,11,14,14,9,12,30,30,16,14,39,17,36,14,28,33,44,26,28,22,24,19,11,14,13,18,13,27,31,12,31,22,30,65,11,7,17,24,15,26,32,37,22,29,23,22,23,22,2,17,25,35,15,18,26,24,17,25,17,18,20,22,20,20,22,25,31,28,24,28,24,25,7,14,24,19,47,20,14,12,29,21,33,38,29,28,40,13,12,44,25,15,13,11,14,16,16,18,30,41,48,38,13,10,15,13,13,20,10,22,18,23,24,24,29,15,19,26,19,29,14,41,36,51,15,21,10,43,68,82,38,16,66,66,88,67,17,6,22,7,20,10,52,20,23,19,21,13,20,31,19,10,5,7,1,8,8,15,8,-1,12,6,7,6,10,13,11,23,29,58,33,17,22,20,18,24,19,20,23,23,20,23,24,22,23,22,21,7,12,6,7,13,13,15,30,19,17,19,19,38,11,13,16,27,15,13,16,25,20,21,18,27,12,23,21,50,16,15,30,28,22,29,28,26,8,10,14,16,86,29,29,38,39,22,47,50,39,43,41,57,23,43,18,14,23,19,27,35,26,18,26,26,21,19,24,32,-5,20,20,34,40,35,13,21,36,22,14]\n",
    "#face_roll_max=[9,10,-7,-17,-17,-17,-5,-2,4,3,-11,-10,-9,-19,-11,-11,-3,0,0,-11,-2,-1,-3,-4,-20,-15,-5,-7,-12,-11,-2,-33,-22,-11,0,-2,-1,-1,-16,-8,-11,-12,-8,-9,-10,-18,-14,-22,0,-7,-3,-3,-7,-1,-4,-4,-2,-3,-5,-14,-3,2,-2,-1,-3,-2,1,-7,-4,-6,-6,-6,-8,-7,-13,-12,-7,0,-3,-9,-4,-1,0,-2,-2,9,2,-6,-5,-5,-4,-5,0,-3,0,3,0,-1,-1,-15,-15,-12,-15,-16,-7,-1,-2,-4,-3,-5,-6,-7,-3,-35,-6,-2,-2,-2,-2,-5,2,3,-3,0,-3,-3,-5,-8,-2,-6,-4,-1,-10,-8,-9,-6,-1,-5,-4,0,-3,-4,-5,-5,6,1,-6,-6,-5,-9,-4,-10,-25,-28,-6,-8,3,-8,-1,-8,3,-8,-4,-14,-11,-6,-12,-8,-15,-14,-16,-4,-24,-7,-3,-8,-19,-4,-13,-8,-3,-28,-11,-14,-5,-11,-5,-8,-6,-7,-2,2,-6,-5,-2,-2,-1,-1,-1,0,-13,-16,-6,-7,-19,4,2,-3,-2,6,0,4,3,6,4,4,3,4,0,3,-20,-3,-13,-14,-13,-2,-7,-4,-2,-11,-8,-8,-4,2,-13,-11,-11,7,4,-7,5,-9,-2,-12,-14,0,2,-2,1,-1,-5,-2,0,-1,-1,-6,-2,-3,-7,-17,-7,-18,-3,-17,2,2,2,1,0,1,-9,-3,-6,-53,-1,-11,-14,-8,-7,-12]\n",
    "#mouth_open_mean=[26.94736842,60.26065574,56.01111111,69.10857143,60.82272727,42.10789474,27.74084507,48.01290323,53.45121951,78.45306122,62.06764706,43.475,35.49230769,0,0,0,0,27.58356164,39.03076923,57.56768707,61.084375,0,20.11111111,27.36290323,54.98120301,58.56408163,51.896,32,0,0,26.41818182,49.38082902,59.87777778,0,25.67047619,56.01906977,59.26136364,0,0,25.37325581,42.54677419,58.35,55.34015748,62.60380952,0,0,25.13690476,51.09386792,75.6525,43.27571429,39.80449438,48.52666667,62.39488189,26.94736842,57.3488189,43.8025,61.735,68.525,74.00263158,29.19189189,0,0,26.94736842,59.72753623,59.58110599,53.70212766,0,15.75,33.4826087,19.46410256,56.25731707,53.976,47.6047619,58.89480519,77.91875,43.98082192,61.48556701,29.05967742,42.49363636,60.01551724,53.87883212,0,0,0,0,0,31.68913043,37.77706422,53.73669725,62.67017544,73.76774194,63.44,38.76842105,24.76724138,47.0675,65.828125,58.64285714,58.35265957,55.40793651,25.3505618,56.57451737,61.1,63.47474747,23.24666667,42.94422111,47.27608696,51.54358974,60.37870968,60.67473684,22.72413793,43.59072848,55.12481203,69.940625,49.09358974,76.60877193,42.61403509,52.75454545,72.004,52.68636364,57.69752066,0,0,42.88215768,56.74705882,55.6578125,62.79519231,0,0,42.10061728,59.7419244,47.68507463,0,22.72413793,29.77959184,45.24588235,52.728125,65.371875,59.80973451,52.38815789,0,32.32237762,56.5625,57.34269663,68.63157895,16.63157895,60.675,0,0,0,24.77065217,46.41034483,59.01934426,56.35853659,68.54,0,0,42.34474708,58.56647059,60.60265487,0,28.09347826,46.24378378,61.2984,47.31625,29.394,24.42419355,60.9,50.05625,74.22073171,59.87614679,43.41311475,0,0,0,30.43559322,18.14545455,53.66335404,64.72557078,41.72033898,0,25.441,58.78382838,53.87883212,0,27.66111111,38.81518987,62.25762712,46.46190476,64.5460177,28.45,58.53193277,59.32608696,58.23876652,0,0,29.15271318,60.72110092,55.07386935,62.79519231,0,32.58979592,55.27518248,61.90740741,57.69752066,0,25.6979798,58.69197324,53.47394366,0,0,0,29.05967742,45.38559322,52.52941176,43.6877551,61.05,64.49530516,22.97692308,0,0,38.24624277,59.18869048,56.58291457,0,0,27.58356164,49.00277778,68.42545455,48.73571429,57.05862069,0,25.1816092,52.00754717,56.88387097,64.39005848,44.82857143,0,39.76222222,58.15460993,53.30128205,0,27.41756757,47.55631068,56.81858974,70.43492063,32.17638889,59.10212766,63.0555,54.73609023,25.63333333,0,0,0,0,26.78571429,58.61404255,62.05,45.4203125,0,0,28.75546875,60.81111111,56.15669291,28.8375,26.38655462,56.07843137,59.70261194,0,0,22.53333333,53.24644269,0,43.35787546,59.68533835,0,0,24.90487805,58.11213115,52.14771242,0]\n",
    "\n",
    "#output=[0,0,0,1,1,0,0,0,-1,0,1,0,1,1,-1,0,1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,0,1,1,0,0,0,1,0,-1,-1,-1,1,-1,-1,0,0,-1,-1,1,1,1,1,1,-1,0,-1,0,0,0,1,0,1,1,1,0,-1,-1,-1,0,1,0,-1,-1,-1,0,-1,1,0,1,-1,1,0,0,0,0,0,0,-1,0,0,1,1,0,0,1,1,-1,1,-1,0,-1,-1,-1,-1,-1,0,-1,-1,-1,-1,-1,1,-1,1,1,1,1,1,1,1,1,1,-1,1,0,1,0,0,1,-1,1,0,-1,1,1,1,-1,0,-1,0,1,0,0,0,0,0,0,1,0,1,0,1,1,1,1,1,0,0,1,1,-1,-1,1,0,1,0,1,-1,-1,-1,-1,-1,0,-1,-1,-1,-1,-1,-1,0,-1,1,-1,1,-1,-1,-1,0,1,1,1,1,1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,0,1,0,-1,-1,1,-1,-1,-1,-1,0,-1,-1,-1,-1,-1,-1,0,-1,0,-1,-1,0,1,0,0,1,1,1,1,1,1,1,0,0,1,0,1,0,0,-1,0,1,0,0,0,0,1,1,-1,1,1,0,0,0,-1,1,-1,-1,-1,-1,1,0,0,0,1,0,1,1,0,-1,-1,0,-1,0,1,0]\n",
    "#video_num=[1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,3,3,3,3,3,4,4,4,4,4,4,4,4,5,5,5,5,6,6,6,6,6,7,7,7,7,7,7,7,8,8,8,8,9,9,9,10,10,10,10,10,10,10,10,10,11,11,11,11,11,12,12,12,12,12,12,12,12,12,12,13,13,13,13,13,13,13,13,14,14,14,14,14,14,14,15,15,15,15,15,15,15,16,16,16,16,16,17,17,17,17,17,18,18,18,18,18,18,19,19,19,19,19,19,19,21,21,21,21,21,21,22,22,22,22,23,23,23,23,23,23,23,23,24,24,24,24,24,24,24,24,24,25,25,25,25,25,25,25,26,26,26,26,27,27,27,27,28,28,28,28,28,28,28,28,28,28,29,29,29,29,29,29,30,30,30,30,31,31,31,31,31,32,32,32,32,32,32,33,33,33,33,33,34,34,34,34,34,35,35,35,35,35,35,36,36,36,36,36,36,36,36,36,37,37,37,37,37,38,38,38,38,38,38,39,39,39,39,39,39,40,40,40,40,41,41,41,41,42,42,42,42,42,42,42,42,42,43,43,43,43,43,43,44,44,44,44,45,45,45,45,45,46,46,46,47,47,47,47,48,48,48,48]\n",
    "\n",
    "\n",
    "#output=np.transpose(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [video, NAQ_Mean, Energy_Max, Energy_Slope_Median, Gaze_Up_Down_Min, Face_Roll_Max, Mouth_Open_Mean, majority_vote]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_excel('/Users/apple/Desktop/USC/*Sem 3/Final_Feature_List.xlsx')\n",
    "columns =['video','NAQ_Mean', 'Energy_Max', 'Energy_Slope_Median','Gaze Up Down Min', 'Face Roll Max', 'Mouth Open Mean','majority vote']\n",
    "data = df[columns]\n",
    "data.columns = ['video','NAQ_Mean', 'Energy_Max', 'Energy_Slope_Median','Gaze_Up_Down_Min', 'Face_Roll_Max', 'Mouth_Open_Mean','majority_vote']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "clf_1=SVC()\n",
    "kf = KFold(47, n_folds=4)\n",
    "C=[0.01,0.1,1,10,100]\n",
    "gamma = [1e-1,1,1e1]\n",
    "\n",
    "features = ['NAQ_Mean', 'Energy_Max', 'Energy_Slope_Median','Gaze_Up_Down_Min', 'Face_Roll_Max', 'Mouth_Open_Mean']\n",
    "target = ['majority_vote']\n",
    "\n",
    "train_data = pd.DataFrame(columns = data.columns)\n",
    "\n",
    "data_normalize = data[features]\n",
    "d = np.array(data_normalize)\n",
    "scaler = preprocessing.StandardScaler().fit(d)\n",
    "dt = scaler.transform(d)\n",
    "dt = pd.DataFrame(dt,columns = features)\n",
    "dt['video'] = data['video']\n",
    "dt['majority_vote'] = data['majority_vote']\n",
    "dt.columsn = columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Validation DataSet 0.328767123288 ***c=*** 0.1\n",
      "Accuracy for Validation DataSet 0.328767123288 ***c=*** 1.0\n",
      "Accuracy for Validation DataSet 0.342465753425 ***c=*** 10.0\n",
      "Accuracy for Validation DataSet 0.424657534247 ***c=*** 100.0\n",
      "Accuracy for Validation DataSet 0.424657534247 ***c=*** 1000.0\n",
      "Accuracy for Training DataSet: 0.515151515152 index= 100.0\n",
      "[ 1.  1.  1.  1.  1.  1. -1. -1. -1.  1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1. -1.  1.  1.  1.  1. -1. -1.\n",
      " -1. -1. -1.  1. -1. -1. -1. -1.  1.  1.  1. -1. -1.  1. -1. -1. -1.  1.\n",
      " -1. -1. -1.  1.  1.  1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "Accuracy for Test Dataset: 0.462686567164 index= 100.0\n",
      "\n",
      "\n",
      "Accuracy for Validation DataSet 0.358208955224 ***c=*** 0.1\n",
      "Accuracy for Validation DataSet 0.358208955224 ***c=*** 1.0\n",
      "Accuracy for Validation DataSet 0.388059701493 ***c=*** 10.0\n",
      "Accuracy for Validation DataSet 0.462686567164 ***c=*** 100.0\n",
      "Accuracy for Validation DataSet 0.417910447761 ***c=*** 1000.0\n",
      "Accuracy for Training DataSet: 0.515151515152 index= 100.0\n",
      "[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  1.  1.  1.  1.  1.  1.  1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1. -1.\n",
      "  1. -1. -1. -1. -1. -1.  1.  1.  1.  1.  1.  1. -1. -1. -1. -1. -1. -1.\n",
      " -1.  1. -1. -1.  1.  1.  1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1.]\n",
      "Accuracy for Test Dataset: 0.424657534247 index= 100.0\n",
      "\n",
      "\n",
      "Accuracy for Validation DataSet 0.298507462687 ***c=*** 0.1\n",
      "Accuracy for Validation DataSet 0.373134328358 ***c=*** 1.0\n",
      "Accuracy for Validation DataSet 0.522388059701 ***c=*** 10.0\n",
      "Accuracy for Validation DataSet 0.522388059701 ***c=*** 100.0\n",
      "Accuracy for Validation DataSet 0.507462686567 ***c=*** 1000.0\n",
      "Accuracy for Training DataSet: 0.470149253731 index= 10.0\n",
      "[-1. -1. -1. -1. -1. -1. -1. -1. -1.  1.  1. -1. -1.  1. -1.  1. -1. -1.\n",
      " -1. -1.  1.  1.  1.  1. -1. -1. -1. -1. -1.  1.  1. -1. -1.  1. -1. -1.\n",
      " -1. -1. -1.  1.  1.  1. -1. -1. -1. -1. -1. -1. -1. -1.  1.  1. -1.  1.\n",
      " -1.  1.  1.  1. -1. -1.  1.  1.  1.  1.  1. -1. -1. -1. -1. -1. -1.]\n",
      "Accuracy for Test Dataset: 0.408450704225 index= 10.0\n",
      "\n",
      "\n",
      "Accuracy for Validation DataSet 0.358208955224 ***c=*** 0.1\n",
      "Accuracy for Validation DataSet 0.358208955224 ***c=*** 1.0\n",
      "Accuracy for Validation DataSet 0.477611940299 ***c=*** 10.0\n",
      "Accuracy for Validation DataSet 0.507462686567 ***c=*** 100.0\n",
      "Accuracy for Validation DataSet 0.582089552239 ***c=*** 1000.0\n",
      "Accuracy for Training DataSet: 0.513888888889 index= 1000.0\n",
      "[-1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1.\n",
      " -1. -1.  1.  1.  1.  1.  1.  1. -1. -1. -1. -1. -1.  1. -1. -1.  1. -1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  0.  1.  1.  1.  1.  1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1.]\n",
      "Accuracy for Test Dataset: 0.393442622951 index= 1000.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Implementing SVM Linear Kernel\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "kf = KFold(47, n_folds=4)\n",
    "clf_linear=SVC()\n",
    "\n",
    "C=[10E-2, 10E-1, 10E0, 10E1, 10E2]\n",
    "for train_index, test_index in kf:\n",
    "    X_train=[]\n",
    "    Y_train=[]\n",
    "    X = pd.DataFrame(columns = features)\n",
    "    Y = pd.DataFrame(columns = target)\n",
    "    \n",
    "    for video_num in train_index[12:] : \n",
    "        X = X.append(data[dt.video == video_num][features])\n",
    "        Y = Y.append(data[dt.video == video_num][target])\n",
    "    \n",
    "    X_train=np.array(X)\n",
    "    Y_train=np.array(Y)\n",
    "    #Y_train=np.transpose(Y)\n",
    "    \n",
    "    \n",
    "    #validation data \n",
    "    X_val = pd.DataFrame(columns = features)\n",
    "    Y_val = pd.DataFrame(columns = target)\n",
    "    \n",
    "    for video_num in train_index[:12] : \n",
    "        X_val = X_val.append(dt[dt.video == video_num][features])\n",
    "        Y_val = Y_val.append(dt[dt.video == video_num][target])\n",
    "    \n",
    "    X_val = np.array(X_val)\n",
    "    Y_val = np.array(Y_val)\n",
    "    #Y_val = np.transpose(Y_val)\n",
    "    \n",
    "    \n",
    "    #test data \n",
    "    X_test = pd.DataFrame(columns = features)\n",
    "    Y_test = pd.DataFrame(columns = target)\n",
    "    \n",
    "    for video_num in test_index : \n",
    "        X_test = X_test.append(dt[dt.video == video_num][features])\n",
    "        Y_test = Y_test.append(dt[dt.video == video_num][target])\n",
    "    \n",
    "    X_test = np.array(X_test)\n",
    "    Y_test = np.array(Y_test)\n",
    "    #Y_test = np.transpose(Y_test)\n",
    "    \n",
    "\n",
    "    \n",
    "    accuracies=[]\n",
    "    max=0\n",
    "    index=0\n",
    "    \n",
    "    for c in C:\n",
    "        clf_linear.set_params(kernel='linear',C=c).fit(X_train,Y_train)\n",
    "        pred=clf_linear.predict(X_val)\n",
    "        print \"Accuracy for Validation DataSet\", accuracy_score(pred,Y_val),\"***c=***\",c\n",
    "        if max<np.asscalar(accuracy_score(pred,Y_val)):\n",
    "            max=np.asscalar(accuracy_score(pred,Y_val))\n",
    "            index=c\n",
    "\n",
    "    \n",
    "    clf_linear.set_params(kernel='linear',C=index).fit(X_train,Y_train)\n",
    "    \n",
    "    pred=clf_linear.predict(X_train)\n",
    "\n",
    "    print \"Accuracy for Training DataSet:\",accuracy_score(pred,Y_train), \"index=\",index\n",
    "    \n",
    "    \n",
    "    pred=clf_linear.predict(X_test)\n",
    "    print pred\n",
    "    print \"Accuracy for Test Dataset:\",accuracy_score(pred,Y_test), \"index=\",index\n",
    "    print \"\\n\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_excel('/Users/apple/Desktop/USC/*Sem 3/Final_Feature_List.xlsx')\n",
    "columns =['video','NAQ_Mean', 'Energy_Max', 'Energy_Slope_Median','Gaze Up Down Min', 'Face Roll Max', 'Mouth Open Mean','majority vote']\n",
    "data = df[columns]\n",
    "data.columns = ['video','NAQ_Mean', 'Energy_Max', 'Energy_Slope_Median','Gaze_Up_Down_Min', 'Face_Roll_Max', 'Mouth_Open_Mean','majority_vote']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "clf_1=SVC()\n",
    "kf = KFold(47, n_folds=4)\n",
    "C=[0.01,0.1,1,10,100]\n",
    "gamma = [1e-1,1,1e1]\n",
    "\n",
    "features = ['NAQ_Mean', 'Energy_Max', 'Energy_Slope_Median','Gaze_Up_Down_Min', 'Face_Roll_Max', 'Mouth_Open_Mean']\n",
    "target = ['majority_vote']\n",
    "\n",
    "train_data = pd.DataFrame(columns = data.columns)\n",
    "\n",
    "data_normalize = data[features]\n",
    "d = np.array(data_normalize)\n",
    "scaler = preprocessing.StandardScaler().fit(d)\n",
    "dt = scaler.transform(d)\n",
    "dt = pd.DataFrame(dt,columns = features)\n",
    "dt['video'] = data['video']\n",
    "dt['majority_vote'] = data['majority_vote']\n",
    "dt.columsn = columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Validation DataSet 0.27397260274 ***c=*** 0.1\n",
      "Accuracy for Validation DataSet 0.315068493151 ***c=*** 1.0\n",
      "Accuracy for Validation DataSet 0.356164383562 ***c=*** 10.0\n",
      "Accuracy for Validation DataSet 0.356164383562 ***c=*** 100.0\n",
      "Accuracy for Validation DataSet 0.356164383562 ***c=*** 1000.0\n",
      "Accuracy for Train Dataset: 0.469696969697 index= 10.0\n",
      "Accuracy for Validation Dataset: 0.356164383562 index= 10.0\n",
      "[ 1.  1.  1. -1.  1.  1.  1.  1.  0.  1.  1.  1.  0.  0.  0.  1.  1. -1.\n",
      " -1. -1. -1. -1. -1.  0.  0. -1. -1.  0.  1. -1.  0.  1.  1.  1. -1.  0.\n",
      "  0.  0.  0. -1. -1. -1. -1. -1. -1. -1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "Accuracy for Test Dataset: 0.388059701493 index= 10.0\n",
      "\n",
      "\n",
      "Accuracy for Validation DataSet 0.34328358209 ***c=*** 0.1\n",
      "Accuracy for Validation DataSet 0.358208955224 ***c=*** 1.0\n",
      "Accuracy for Validation DataSet 0.388059701493 ***c=*** 10.0\n",
      "Accuracy for Validation DataSet 0.388059701493 ***c=*** 100.0\n",
      "Accuracy for Validation DataSet 0.388059701493 ***c=*** 1000.0\n",
      "Accuracy for Train Dataset: 0.469696969697 index= 10.0\n",
      "Accuracy for Validation Dataset: 0.388059701493 index= 10.0\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0. -1. -1.  0. -1.  0. -1.  0.  0.\n",
      "  0.  1.  1.  1.  1.  1.  1.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "  1. -1.  0.  0.  0.  0.  1.  1.  1.  1.  1.  1.  1.  0.  0.  0.  0.  0.\n",
      "  0.  1.  1.  1. -1.  1.  1. -1.  0. -1.  0.  0.  1.  1. -1.  0. -1. -1.\n",
      "  0.]\n",
      "Accuracy for Test Dataset: 0.356164383562 index= 10.0\n",
      "\n",
      "\n",
      "Accuracy for Validation DataSet 0.313432835821 ***c=*** 0.1\n",
      "Accuracy for Validation DataSet 0.417910447761 ***c=*** 1.0\n",
      "Accuracy for Validation DataSet 0.432835820896 ***c=*** 10.0\n",
      "Accuracy for Validation DataSet 0.432835820896 ***c=*** 100.0\n",
      "Accuracy for Validation DataSet 0.432835820896 ***c=*** 1000.0\n",
      "Accuracy for Train Dataset: 0.432835820896 index= 10.0\n",
      "Accuracy for Validation Dataset: 0.432835820896 index= 10.0\n",
      "[ 0. -1. -1. -1.  0.  0. -1. -1. -1. -1.  0. -1. -1.  0. -1.  0. -1. -1.\n",
      " -1. -1.  1.  1.  1.  1.  1. -1. -1.  1. -1.  1. -1. -1.  1. -1.  1. -1.\n",
      " -1. -1. -1.  1.  1. -1.  1. -1. -1. -1.  1.  1. -1. -1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1. -1.  1.  1.  1.  1.  1.  1. -1. -1. -1.  1. -1. -1.]\n",
      "Accuracy for Test Dataset: 0.380281690141 index= 10.0\n",
      "\n",
      "\n",
      "Accuracy for Validation DataSet 0.507462686567 ***c=*** 0.1\n",
      "Accuracy for Validation DataSet 0.492537313433 ***c=*** 1.0\n",
      "Accuracy for Validation DataSet 0.492537313433 ***c=*** 10.0\n",
      "Accuracy for Validation DataSet 0.492537313433 ***c=*** 100.0\n",
      "Accuracy for Validation DataSet 0.492537313433 ***c=*** 1000.0\n",
      "Accuracy for Train Dataset: 0.486111111111 index= 0.1\n",
      "Accuracy for Validation Dataset: 0.507462686567 index= 0.1\n",
      "[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1.  1. -1. -1. -1.  1.\n",
      " -1. -1.  1. -1. -1.  1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1. -1.\n",
      "  1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1.  1. -1. -1.]\n",
      "Accuracy for Test Dataset: 0.360655737705 index= 0.1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Implementing SVM Linear on Acoustic Only\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "kf = KFold(47, n_folds=4)\n",
    "clf_linear=SVC()\n",
    "features = ['NAQ_Mean', 'Energy_Max', 'Energy_Slope_Median']\n",
    "\n",
    "C=[10E-2, 10E-1, 10E0, 10E1, 10E2]\n",
    "for train_index, test_index in kf:\n",
    "    #X_train=[]\n",
    "    #Y_train=[]\n",
    "    X = pd.DataFrame(columns = features)\n",
    "    Y = pd.DataFrame(columns = target)\n",
    "    \n",
    "    for video_num in train_index[12:] : \n",
    "        X = X.append(dt[dt.video == video_num][features])\n",
    "        Y = Y.append(dt[dt.video == video_num][target])\n",
    "    \n",
    "    X_train=np.array(X)\n",
    "    Y_train=np.array(Y)\n",
    "    #Y_train=np.transpose(Y)\n",
    "    #print X\n",
    "    \n",
    "    #validation data \n",
    "    X_val = pd.DataFrame(columns = features)\n",
    "    Y_val = pd.DataFrame(columns = target)\n",
    "    \n",
    "    for video_num in train_index[:12] : \n",
    "        X_val = X_val.append(dt[dt.video == video_num][features])\n",
    "        Y_val = Y_val.append(dt[dt.video == video_num][target])\n",
    "    \n",
    "    X_val = np.array(X_val)\n",
    "    Y_val = np.array(Y_val)\n",
    "    #Y_val = np.transpose(Y_val)\n",
    "    \n",
    "    \n",
    "    #test data \n",
    "    X_test = pd.DataFrame(columns = features)\n",
    "    Y_test = pd.DataFrame(columns = target)\n",
    "    \n",
    "    for video_num in test_index : \n",
    "        X_test = X_test.append(dt[dt.video == video_num][features])\n",
    "        Y_test = Y_test.append(dt[dt.video == video_num][target])\n",
    "    \n",
    "    X_test = np.array(X_test)\n",
    "    Y_test = np.array(Y_test)\n",
    "    #Y_test = np.transpose(Y_test)\n",
    "    \n",
    "    \n",
    "    accuracies=[]\n",
    "    max=0\n",
    "    index=0\n",
    "    \n",
    "    for c in C:\n",
    "        clf_linear.set_params(kernel='linear',C=c).fit(X_train,Y_train)\n",
    "        pred=clf_linear.predict(X_val)\n",
    "        print \"Accuracy for Validation DataSet\", accuracy_score(pred,Y_val),\"***c=***\",c\n",
    "        if max<np.asscalar(accuracy_score(pred,Y_val)):\n",
    "            max=np.asscalar(accuracy_score(pred,Y_val))\n",
    "            index=c\n",
    "\n",
    "    \n",
    "    clf_linear.set_params(kernel='linear',C=index).fit(X_train,Y_train)\n",
    "    \n",
    "    pred=clf_linear.predict(X_train)\n",
    "\n",
    "    print \"Accuracy for Train Dataset:\", accuracy_score(pred,Y_train), \"index=\",index\n",
    "    pred_val=clf_linear.predict(X_val)\n",
    "    print \"Accuracy for Validation Dataset:\", accuracy_score(pred_val,Y_val), \"index=\",index\n",
    "    \n",
    "    pred=clf_linear.predict(X_test)\n",
    "    print pred\n",
    "    \n",
    "    print \"Accuracy for Test Dataset:\",accuracy_score(pred,Y_test), \"index=\",index\n",
    "    print \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Validation DataSet 0.369863013699 ***c=*** 0.1\n",
      "Accuracy for Validation DataSet 0.369863013699 ***c=*** 1.0\n",
      "Accuracy for Validation DataSet 0.369863013699 ***c=*** 10.0\n",
      "Accuracy for Validation DataSet 0.369863013699 ***c=*** 100.0\n",
      "Accuracy for Validation DataSet 0.369863013699 ***c=*** 1000.0\n",
      "Accuracy for Training DataSet: 0.477272727273 index= 0.1\n",
      "Accuracy for Validation Dataset: 0.369863013699 index= 0.1\n",
      "[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "Accuracy for Test Dataset: 0.388059701493 index= 0.1\n",
      "\n",
      "\n",
      "Accuracy for Validation DataSet 0.388059701493 ***c=*** 0.1\n",
      "Accuracy for Validation DataSet 0.417910447761 ***c=*** 1.0\n",
      "Accuracy for Validation DataSet 0.417910447761 ***c=*** 10.0\n",
      "Accuracy for Validation DataSet 0.402985074627 ***c=*** 100.0\n",
      "Accuracy for Validation DataSet 0.402985074627 ***c=*** 1000.0\n",
      "Accuracy for Training DataSet: 0.462121212121 index= 1.0\n",
      "Accuracy for Validation Dataset: 0.417910447761 index= 1.0\n",
      "[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1.  0. -1. -1. -1. -1. -1.  0.  0. -1. -1.  1. -1. -1.  0.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1.  0.  1.  0. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1.\n",
      " -1.]\n",
      "Accuracy for Test Dataset: 0.369863013699 index= 1.0\n",
      "\n",
      "\n",
      "Accuracy for Validation DataSet 0.432835820896 ***c=*** 0.1\n",
      "Accuracy for Validation DataSet 0.358208955224 ***c=*** 1.0\n",
      "Accuracy for Validation DataSet 0.358208955224 ***c=*** 10.0\n",
      "Accuracy for Validation DataSet 0.358208955224 ***c=*** 100.0\n",
      "Accuracy for Validation DataSet 0.358208955224 ***c=*** 1000.0\n",
      "Accuracy for Training DataSet: 0.388059701493 index= 0.1\n",
      "Accuracy for Validation Dataset: 0.432835820896 index= 0.1\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1. -1.  1.  1.  1.  1.  0.  0.\n",
      "  0.  1.  0.  1.  0.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  0.  1.  1.  1.  1.  0.  1.  1.  1.  0.  1.  0.  1.  0.  0.  0.  1.\n",
      "  1.  1.  0.  0.  0.  1.  1.  1.  0.  1.  1.  0.  0.  0.  1.  1.  1.]\n",
      "Accuracy for Test Dataset: 0.323943661972 index= 0.1\n",
      "\n",
      "\n",
      "Accuracy for Validation DataSet 0.373134328358 ***c=*** 0.1\n",
      "Accuracy for Validation DataSet 0.358208955224 ***c=*** 1.0\n",
      "Accuracy for Validation DataSet 0.358208955224 ***c=*** 10.0\n",
      "Accuracy for Validation DataSet 0.358208955224 ***c=*** 100.0\n",
      "Accuracy for Validation DataSet 0.358208955224 ***c=*** 1000.0\n",
      "Accuracy for Training DataSet: 0.451388888889 index= 0.1\n",
      "Accuracy for Validation Dataset: 0.373134328358 index= 0.1\n",
      "[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  0.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1. -1. -1. -1.\n",
      "  0.  0. -1.  1.  1.  0. -1.  1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1.]\n",
      "Accuracy for Test Dataset: 0.393442622951 index= 0.1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Implementing SVM Linear Kernel for Visual ONLY Data\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "kf = KFold(47, n_folds=4)\n",
    "clf_linear=SVC()\n",
    "features = ['Gaze_Up_Down_Min', 'Face_Roll_Max', 'Mouth_Open_Mean']\n",
    "C=[10E-2, 10E-1, 10E0, 10E1, 10E2]\n",
    "for train_index, test_index in kf:\n",
    "    X_train=[]\n",
    "    Y_train=[]\n",
    "    X = pd.DataFrame(columns = features)\n",
    "    Y = pd.DataFrame(columns = target)\n",
    "    \n",
    "    for video_num in train_index[12:] : \n",
    "        X = X.append(dt[dt.video == video_num][features])\n",
    "        Y = Y.append(dt[dt.video == video_num][target])\n",
    "    \n",
    "    X_train=np.array(X)\n",
    "    Y_train=np.array(Y)\n",
    "    #Y_train=np.transpose(Y)\n",
    "    \n",
    "    \n",
    "    #validation data \n",
    "    X_val = pd.DataFrame(columns = features)\n",
    "    Y_val = pd.DataFrame(columns = target)\n",
    "    \n",
    "    for video_num in train_index[:12] : \n",
    "        X_val = X_val.append(dt[dt.video == video_num][features])\n",
    "        Y_val = Y_val.append(dt[dt.video == video_num][target])\n",
    "    \n",
    "    X_val = np.array(X_val)\n",
    "    Y_val = np.array(Y_val)\n",
    "    #Y_val = np.transpose(Y_val)\n",
    "    \n",
    "    \n",
    "    #test data \n",
    "    X_test = pd.DataFrame(columns = features)\n",
    "    Y_test = pd.DataFrame(columns = target)\n",
    "    \n",
    "    for video_num in test_index : \n",
    "        X_test = X_test.append(dt[dt.video == video_num][features])\n",
    "        Y_test = Y_test.append(dt[dt.video == video_num][target])\n",
    "    \n",
    "    X_test = np.array(X_test)\n",
    "    Y_test = np.array(Y_test)\n",
    "    #Y_test = np.transpose(Y_test)\n",
    "    \n",
    "    accuracies=[]\n",
    "    max=0\n",
    "    index=0\n",
    "    \n",
    "    for c in C:\n",
    "        clf_linear.set_params(kernel='linear',C=c).fit(X_train,Y_train)\n",
    "        pred=clf_linear.predict(X_val)\n",
    "        print \"Accuracy for Validation DataSet\", accuracy_score(pred,Y_val),\"***c=***\",c\n",
    "        if max<np.asscalar(accuracy_score(pred,Y_val)):\n",
    "            max=np.asscalar(accuracy_score(pred,Y_val))\n",
    "            index=c\n",
    "\n",
    "    \n",
    "    clf_linear.set_params(kernel='linear',C=index).fit(X_train,Y_train)\n",
    "    \n",
    "    pred=clf_linear.predict(X_train)\n",
    "\n",
    "    print \"Accuracy for Training DataSet:\",accuracy_score(pred,Y_train), \"index=\",index\n",
    "\n",
    "    pred_val=clf_linear.predict(X_val)\n",
    "    print \"Accuracy for Validation Dataset:\", accuracy_score(pred_val,Y_val), \"index=\",index\n",
    "    \n",
    "    pred=clf_linear.predict(X_test)\n",
    "    print pred\n",
    "    print \"Accuracy for Test Dataset:\",accuracy_score(pred,Y_test), \"index=\",index\n",
    "    print \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Implementing SVM-RBF Kernel\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import KFold\n",
    "kf = KFold(47, n_folds=4)\n",
    "clf_rbf=SVC()\n",
    "\n",
    "C=[10E-2, 10E-1, 10E0, 10E1, 10E2]\n",
    "g=[1e-1, 1, 1e1]\n",
    "\n",
    "for train_index, test_index in kf:\n",
    "     X_train=[]\n",
    "    Y_train=[]\n",
    "    X = pd.DataFrame(columns = features)\n",
    "    Y = pd.DataFrame(columns = target)\n",
    "    \n",
    "    for video_num in train_index[12:] : \n",
    "        X = X.append(dt[dt.video == video_num][features])\n",
    "        Y = Y.append(dt[dt.video == video_num][target])\n",
    "    \n",
    "    X_train=np.array(X)\n",
    "    Y_train=np.array(Y)\n",
    "    #Y_train=np.transpose(Y)\n",
    "    \n",
    "    \n",
    "    #validation data \n",
    "    X_val = pd.DataFrame(columns = features)\n",
    "    Y_val = pd.DataFrame(columns = target)\n",
    "    \n",
    "    for video_num in train_index[:12] : \n",
    "        X_val = X_val.append(dt[dt.video == video_num][features])\n",
    "        Y_val = Y_val.append(dt[dt.video == video_num][target])\n",
    "    \n",
    "    X_val = np.array(X_val)\n",
    "    Y_val = np.array(Y_val)\n",
    "    #Y_val = np.transpose(Y_val)\n",
    "    \n",
    "    \n",
    "    #test data \n",
    "    X_test = pd.DataFrame(columns = features)\n",
    "    Y_test = pd.DataFrame(columns = target)\n",
    "    \n",
    "    for video_num in test_index : \n",
    "        X_test = X_test.append(dt[dt.video == video_num][features])\n",
    "        Y_test = Y_test.append(dt[dt.video == video_num][target])\n",
    "    \n",
    "    X_test = np.array(X_test)\n",
    "    Y_test = np.array(Y_test)\n",
    "    #Y_test = np.transpose(Y_test)\n",
    "    \n",
    "    accuracies=[]\n",
    "    max=0\n",
    "    index=0\n",
    "    Gamma=0\n",
    "    for c in C:\n",
    "        for gamma in g:\n",
    "            clf_rbf.set_params(kernel='rbf',C=c,gamma=gamma).fit(train_data,train_output_data)\n",
    "    \n",
    "            pred=clf_rbf.predict(X_val)\n",
    "            print \"Accuracy for Validation\",accuracy_score(pred,Y_val),\"***c=***\",c, \"***gamma***\",gamma\n",
    "        \n",
    "            if max<np.asscalar(accuracy_score(pred,Y_val)):\n",
    "                max=np.asscalar(accuracy_score(pred,Y_val))\n",
    "                index=c\n",
    "                Gamma=gamma\n",
    "\n",
    "    \n",
    "    clf_rbf.set_params(kernel='rbf',C=index,gamma=Gamma).fit(train_data,train_output_data)\n",
    "\n",
    "    pred=clf_rbf.predict(train_data)\n",
    "    print \"\\n\"\n",
    "    print \"Accuracy for Training DataSet:\",accuracy_score(pred,train_output_data), \"index=\",index,\"**gamma=***\",Gamma\n",
    "    \n",
    "    X_test=[]\n",
    "    Y_test=[]\n",
    "    for i in test_index:\n",
    "        #print i\n",
    "        X_test.append([naq_mean[i],energy_max[i],energy_slope_max[i],gaze_up_down_min[i],face_roll_max[i],mouth_open_mean[i]])\n",
    "        Y_test.append(output[i])\n",
    "    X_test=np.array(X_test)\n",
    "    Y_test=np.array(Y_test)\n",
    "    X_text=np.transpose(X_test)\n",
    "    Y_test=np.transpose(Y_test)\n",
    "    \n",
    "    pred=clf_rbf.predict(X_test)\n",
    "\n",
    "    print \"Accuracy for test Dataset:\",accuracy_score(pred,Y_test), \"index=\",index\n",
    "    print \"\\n\"\n",
    "    \n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Normalized RBF:\n",
    "#RBF Kernel\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "scaler = StandardScaler() \n",
    "\n",
    "kf = KFold(280, n_folds=4)\n",
    "clf_rbf=SVC()\n",
    "\n",
    "C=[10E-2, 10E-1, 10E0, 10E1, 10E2]\n",
    "g=[1e-1, 1, 1e1]\n",
    "\n",
    "for train_index, test_index in kf:\n",
    "    X=[]\n",
    "    Y=[]\n",
    "    for i in train_index[71:]:\n",
    "        #print i\n",
    "        X.append([naq_mean[i],energy_max[i],energy_slope_max[i],gaze_up_down_min[i],face_roll_max[i],mouth_open_mean[i]])\n",
    "        Y.append(output[i])\n",
    "    \n",
    "    train_data=np.array(X)\n",
    "    #train_data=np.transpose(train_data)\n",
    "    output_data=np.array(Y)\n",
    "    train_output_data=np.transpose(output_data)\n",
    "    \n",
    "    scaler.fit(train_data) \n",
    "    X_train = scaler.transform(train_data)\n",
    "    \n",
    "    validation_index=train_index[:70]\n",
    "    X_val=[]\n",
    "    Y_val=[]\n",
    "    for i in validation_index:\n",
    "        X_val.append([naq_mean[i],energy_max[i],energy_slope_max[i],gaze_up_down_min[i],face_roll_max[i],mouth_open_mean[i]])\n",
    "        Y_val.append(output[i])\n",
    "    X_val=np.array(X_val)\n",
    "\n",
    "    X_val = scaler.transform(X_val)\n",
    "    #X_val=np.transpose(X_val)\n",
    "    Y_val=np.array(Y_val)\n",
    "    Y_val=np.transpose(Y_val)\n",
    "    accuracies=[]\n",
    "    max=0\n",
    "    index=0\n",
    "    Gamma=0\n",
    "    for c in C:\n",
    "        for gamma in g:\n",
    "            clf_rbf.set_params(kernel='rbf',C=c,gamma=gamma).fit(X_train,train_output_data)\n",
    "    \n",
    "            pred=clf_rbf.predict(X_val)\n",
    "            print \"Accuracy for Validation\",accuracy_score(pred,Y_val),\"***c=***\",c, \"***gamma***\",gamma\n",
    "        \n",
    "            if max<np.asscalar(accuracy_score(pred,Y_val)):\n",
    "                max=np.asscalar(accuracy_score(pred,Y_val))\n",
    "                index=c\n",
    "                Gamma=gamma\n",
    "\n",
    "    \n",
    "    clf_rbf.set_params(kernel='rbf',C=index,gamma=Gamma).fit(X_train,train_output_data)\n",
    "\n",
    "    pred=clf_rbf.predict(X_train)\n",
    "    print \"\\n\"\n",
    "    print \"Accuracy for Training DataSet:\",accuracy_score(pred,train_output_data), \"index=\",index,\"**gamma=***\",Gamma\n",
    "    pred=clf_rbf.predict(X_val)\n",
    "    print \"Accuracy for Validation\",accuracy_score(pred,Y_val),\"***c=***\",index, \"***gamma***\",gamma\n",
    "    \n",
    "    X_test=[]\n",
    "    Y_test=[]\n",
    "    for i in test_index:\n",
    "        #print i\n",
    "        X_test.append([naq_mean[i],energy_max[i],energy_slope_max[i],gaze_up_down_min[i],face_roll_max[i],mouth_open_mean[i]])\n",
    "        Y_test.append(output[i])\n",
    "    X_test=np.array(X_test)\n",
    "    Y_test=np.array(Y_test)\n",
    "\n",
    "    Y_test=np.transpose(Y_test)\n",
    "    \n",
    "    X_test = scaler.transform(X_test)\n",
    "    pred=clf_rbf.predict(X_test)\n",
    "\n",
    "    print \"Accuracy for test Dataset:\",accuracy_score(pred,Y_test), \"index=\",index\n",
    "    print \"\\n\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      "Accuracy for Training DataSet: 0.49756097561\n",
      "Accuracy for test Dataset: 0.492537313433\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Accuracy for Training DataSet: 0.542713567839\n",
      "Accuracy for test Dataset: 0.369863013699\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Accuracy for Training DataSet: 0.517412935323\n",
      "Accuracy for test Dataset: 0.535211267606\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Accuracy for Training DataSet: 0.530805687204\n",
      "Accuracy for test Dataset: 0.409836065574\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Naive Bayes Classifier\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.cross_validation import KFold\n",
    "kf = KFold(47, n_folds=4)\n",
    "clf_nb = GaussianNB()\n",
    "#clf.fit(features_train, labels_train)\n",
    "features = ['NAQ_Mean', 'Energy_Max', 'Energy_Slope_Median','Gaze_Up_Down_Min', 'Face_Roll_Max', 'Mouth_Open_Mean']\n",
    "\n",
    "\n",
    "for train_index, test_index in kf:\n",
    "    X_train=[]\n",
    "    Y_train=[]\n",
    "    X = pd.DataFrame(columns = features)\n",
    "    Y = pd.DataFrame(columns = target)\n",
    "    \n",
    "    for video_num in train_index : \n",
    "        X = X.append(dt[dt.video == video_num][features])\n",
    "        Y = Y.append(dt[dt.video == video_num][target])\n",
    "    \n",
    "    X_train=np.array(X)\n",
    "    Y_train=np.array(Y)\n",
    "    #Y_train=np.transpose(Y)\n",
    "    \n",
    "    \n",
    "    #test data \n",
    "    X_test = pd.DataFrame(columns = features)\n",
    "    Y_test = pd.DataFrame(columns = target)\n",
    "    \n",
    "    for video_num in test_index : \n",
    "        X_test = X_test.append(dt[dt.video == video_num][features])\n",
    "        Y_test = Y_test.append(dt[dt.video == video_num][target])\n",
    "    \n",
    "    X_test = np.array(X_test)\n",
    "    Y_test = np.array(Y_test)\n",
    "    #Y_test = np.transpose(Y_test)\n",
    "    \n",
    "    accuracies=[]\n",
    "    max=0\n",
    "    index=0\n",
    "    Gamma=0\n",
    "\n",
    "    clf_nb.fit(X_train,Y_train)\n",
    "    pred=clf_nb.predict(X_train)\n",
    "    \n",
    "    print \"\\n\"\n",
    "    print \"Accuracy for Training DataSet:\",accuracy_score(pred,Y_train)\n",
    "    \n",
    "    \n",
    "    \n",
    "    pred=clf_nb.predict(X_test)\n",
    "\n",
    "    print \"Accuracy for test Dataset:\",accuracy_score(pred,Y_test)\n",
    "    print \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "Accuracy for Training DataSet: 0.536585365854\n",
      "Accuracy for test Dataset: 0.44776119403\n",
      "\n",
      "\n",
      "3\n",
      "Accuracy for Training DataSet: 0.577889447236\n",
      "Accuracy for test Dataset: 0.438356164384\n",
      "\n",
      "\n",
      "3\n",
      "Accuracy for Training DataSet: 0.557213930348\n",
      "Accuracy for test Dataset: 0.366197183099\n",
      "\n",
      "\n",
      "3\n",
      "Accuracy for Training DataSet: 0.587677725118\n",
      "Accuracy for test Dataset: 0.508196721311\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Neural Network implementation\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "features = ['NAQ_Mean', 'Energy_Max', 'Energy_Slope_Median','Gaze_Up_Down_Min', 'Face_Roll_Max', 'Mouth_Open_Mean']\n",
    "h=[3,5,7,9,11,13,15,17,19,21]\n",
    "h=[3]\n",
    "scaler = StandardScaler() \n",
    "kf = KFold(47, n_folds=4)\n",
    "\n",
    "clf_neural = MLPClassifier(solver='lbfgs',activation='relu', alpha=1e-5,hidden_layer_sizes=(3,),max_iter=200, random_state=5)\n",
    "for train_index, test_index in kf:\n",
    "    X_train=[]\n",
    "    Y_train=[]\n",
    "    X = pd.DataFrame(columns = features)\n",
    "    Y = pd.DataFrame(columns = target)\n",
    "    \n",
    "    for video_num in train_index : \n",
    "        X = X.append(dt[dt.video == video_num][features])\n",
    "        Y = Y.append(dt[dt.video == video_num][target])\n",
    "    \n",
    "    X_train=np.array(X)\n",
    "    Y_train=np.array(Y)\n",
    "    #Y_train=np.transpose(Y)\n",
    "    \n",
    "    \n",
    "    #test data \n",
    "    X_test = pd.DataFrame(columns = features)\n",
    "    Y_test = pd.DataFrame(columns = target)\n",
    "    \n",
    "    for video_num in test_index : \n",
    "        X_test = X_test.append(dt[dt.video == video_num][features])\n",
    "        Y_test = Y_test.append(dt[dt.video == video_num][target])\n",
    "    \n",
    "    X_test = np.array(X_test)\n",
    "    Y_test = np.array(Y_test)\n",
    "    #Y_test = np.transpose(Y_test)\n",
    "    \n",
    "    #scaler.fit(X_train) \n",
    "    #X_train = scaler.transform(X_train) \n",
    "    for i in h:\n",
    "        print i\n",
    "        clf_neural = MLPClassifier(solver='lbfgs',activation='relu', alpha=1e-5,hidden_layer_sizes=(i,),max_iter=200, random_state=5)\n",
    "\n",
    "        clf_neural.fit(X_train,Y_train)\n",
    "\n",
    "        pred=clf_neural.predict(X_train)\n",
    "        print \"Accuracy for Training DataSet:\",accuracy_score(pred,Y_train)\n",
    "    \n",
    "    \n",
    "    \n",
    "        pred=clf_neural.predict(X_test)\n",
    "    \n",
    "        print \"Accuracy for test Dataset:\",accuracy_score(pred,Y_test)\n",
    "    print \"\\n\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
