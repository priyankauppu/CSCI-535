{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#variable declaration\n",
    "\n",
    "fnames=['video1(00h00m27s-00h01m01s).feat','video2(00h00m37s-00h01m24s).feat','video3(00h00m18s-00h00m49s).feat','video4(00h00m12s-00h00m48s).feat', 'video5(00h01m08s-00h01m37s).feat', 'video6(00h00m25s-00h01m06s).feat', 'video7(00h00m11s-00h00m45s).feat', 'video8(00h05m29s-00h05m58s).feat', 'video9(00h03m52s-00h04m31s).feat', 'video10(00h00m29s-00h00m58s).feat', 'video11(00h02m38s-00h03m15s).feat', 'video12(00h00m39s-00h01m14s).feat', 'video13(00h00m13s-00h00m44s).feat', 'video14(00h00m00s-00h00m29s).feat', 'video15(00h00m21s-00h00m56s).feat', 'video16(00h00m20s-00h00m50s).feat', 'video17(00h02m09s-00h02m28s).feat', 'video18(00h00m33s-00h01m04s).feat', 'video19(00h00m42s-00h01m12s).feat', 'video20(00h00m51s-00h01m11s).feat', 'video21(00h00m11s-00h00m43s).feat', 'video22(00h00m34s-00h01m10s).feat', 'video23(00h00m15s-00h00m54s).feat', 'video24(00h00m01s-00h00m35s).feat', 'video25(00h00m15s-00h00m46s).feat', 'video26(00h00m48s-00h01m24s).feat', 'video27(00h01m16s-00h01m47s).feat', 'video28(00h00m03s-00h00m36s).feat', 'video29(00h00m00s-00h00m29s).feat', 'video30(00h02m01s-00h02m33s).feat', 'video31(00h00m06s-00h00m42s).feat', 'video32(00h00m15s-00h00m45s).feat', 'video33(00h00m17s-00h01m15s).feat', 'video34(00h00m05s-00h00m33s).feat', 'video35(00h00m19s-00h00m56s).feat', 'video36(00h00m09s-00h00m46s).feat', 'video37(00h01m24s-00h01m58s).feat', 'video38(00h00m23s-00h00m57s).feat', 'video39(00h00m38s-00h01m09s).feat', 'video40(00h00m12s-00h00m38s).feat', 'video41(00h00m01s-00h00m27s).feat', 'video42(00h00m35s-00h01m22s).feat', 'video43(00h00m15s-00h00m52s).feat', 'video44(00h00m11s-00h00m40s).feat', 'video45(00h00m06s-00h00m37s).feat', 'video46(00h00m27s-00h01m06s).feat', 'video47(00h00m18s-00h00m53s).feat', 'video48(00h00m12s-00h00m46s).feat',\n",
    "]\n",
    "\n",
    "#stopterms\n",
    "stop_words_list=[[1,777],[1,395], [1,894], [1,458], [1,726], [1,250], [2,291], [2,741], [2,160], [2,194], [2,539], [2,191], [2,122], [2,1138], [2,472], [2,457], [2,449], [3,250], [3,258], [3,976], [3,1327], [3,303], [4,96], [4,206], [4,440], [4,813], [4,249], [4,332], [4,656], [4,778], [5,264], [5,640], [5,1059], [5,830], [6,358], [6,714], [6,1195], [6,282], [6,1648], [7,294], [7,203], [7,539], [7,418], [7,790], [7,338], [7,898], [8,577], [8,1422], [8,536], [8,466], [9,1385], [9,681], [9,1946], [10,388], [10,419], [10,132], [10,462], [10,158], [10,123], [10,510], [10,568], [10,43], [11,462], [11,273], [11,864], [11,1790], [11,311], [12,134], [12,300], [12,254], [12,541], [12,163], [12,272], [12,508], [12,314], [12,479], [12,800], [13,214], [13,363], [13,773], [13,504], [13,115], [13,117], [13,552], [13,480], [14,39], [14,299], [14,723], [14,719], [14,375], [14,198], [14,330], [15,122], [15,378], [15,260], [15,207], [15,367], [15,1254], [15,901], [16,293], [16,860], [16,206], [16,329], [16,1226], [17,663], [17,148], [17,127], [17,515], [17,416], [18,205], [18,1002], [18,877], [18,423], [18,512], [18,375], [19,766], [19,254], [19,165], [19,217], [19,854], [19,479], [19,321], [21,812], [21,221], [21,424], [21,431], [21,735], [21,514], [22,535], [22,966], [22,600], [22,1398], [23,209], [23,320], [23,558], [23,638], [23,630], [23,747], [23,669], [23,360], [24,484], [24,343], [24,591], [24,314], [24,59], [24,162], [24,191], [24,605], [24,704], [25,312], [25,192], [25,1013], [25,273], [25,446], [25,506], [25,342], [26,863], [26,563], [26,610], [26,1465], [27,371], [27,1417], [27,953], [27,605], [28,174], [28,203], [28,222], [28,371], [28,272], [28,360], [28,443], [28,591], [28,267], [28,401], [29,205], [29,144], [29,531], [29,727], [29,598], [29,621], [30,340], [30,1007], [30,1024], [30,847], [31,752], [31,790], [31,585], [31,426], [31,1134], [32,428], [32,393], [32,227], [32,761], [32,514], [32,694], [33,1093], [33,903], [33,1656], [33,1936], [33,323], [34,499], [34,454], [34,450], [34,513], [34,896], [35,336], [35,995], [35,970], [35,363], [35,441], [35,765], [36,265], [36,488], [36,140], [36,200], [36,172], [36,885], [36,366], [36,629], [36,634], [37,584], [37,558], [37,1088], [37,454], [37,713], [38,250], [38,717], [38,365], [38,183], [38,1530], [38,389], [39,296], [39,529], [39,201], [39,566], [39,903], [39,692], [40,608], [40,934], [40,580], [40,441], [41,505], [41,682], [41,1037], [41,415], [42,486], [42,155], [42,665], [42,438], [42,309], [42,760], [42,310], [42,766], [42,903], [43,380], [43,782], [43,430], [43,772], [43,423], [43,1000], [44,436], [44,899], [44,421], [44,1190], [45,403], [45,508], [45,900], [45,837], [45,449], [46,115], [46,3056], [46,785], [47,1101], [47,1555], [47,642], [47,287], [48,280], [48,1013], [48,1190], [48,921],\n",
    "]\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file_number=21\n",
    "for findex in fnames[20:]:\n",
    "    f=\"/Users/apple/Desktop/USC/*Sem 3/CSCI 535/Homework/Homework_1/features/AcousticFeatures/\"+findex\n",
    "    fo=open(f,\"r+\")\n",
    "    str=fo.read();\n",
    "    mylist = str.split('\\n')\n",
    "\n",
    "    acoustic_features=[] #holds the input acoustic file as a list\n",
    "    \n",
    "    for i in mylist:\n",
    "        acoustic_features.append(i.split(','))\n",
    "        \n",
    "    stop_points=[]\n",
    "    #print str,i\n",
    "    for i in stop_words_list:\n",
    "        if i[0]==file_number:\n",
    "            stop_points.append(i[1])\n",
    "    \n",
    "    start=0\n",
    "    end=stop_points[0]\n",
    "    i=0\n",
    "    while i<len(stop_points):#Summarising for segment\n",
    "        feature=[]\n",
    "        for j in acoustic_features[start:end]:\n",
    "            f=j[6]        #change the FEATURE HERE\n",
    "            total_break=0\n",
    "            if f==[''] or f==\" \" or f.lower()=='-inf'or f.lower()=='inf' or f.lower()=='nan':\n",
    "                total_break+=1\n",
    "            else:    \n",
    "                feature.append(float(f))\n",
    "        \n",
    "        print np.mean(feature),np.median(feature),min(feature),max(feature),np.std(feature),file_number,end-start\n",
    "\n",
    "        start=end+1\n",
    "        if i+1<len(stop_points):\n",
    "            end=stop_points[i+1]+start\n",
    "        i+=1\n",
    "        \n",
    "    file_number+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "naq_mean=np.array([0.233433266,0.271499846,0.194656705,0.181773902,0.241251601,0.265086984,0.14201744,0.150883289,0.09933675,0.177312719,0.145586967,0.157967267,0.102980098,0.158541599,0.133451922,0.169483455,0.162872673,0.065124556,0.040800376,0.095198734,0.111916183,0.094448743,0.167350417,0.14658334,0.077898064,0.133047281,0.130726566,0.129402373,0.190965075,0.134988408,0.210779379,0.284076145,0.282775033,0.279064789,0.088195386,0.075816688,0.085474097,0.089118429,0.09717783,0.196630779,0.136723469,0.173573746,0.143170418,0.155082261,0.197227479,0.18364364,0.17065009,0.150120044,0.158545951,0.190377017,0.095941138,0.088948628,0.095032433,0.184354567,0.146712652,0.158073212,0.166655461,0.198684544,0.204016179,0.190650649,0.172986713,0.162084907,0.097096912,0.111067612,0.107732716,0.113447461,0.108421046,0.148826254,0.123528583,0.14933202,0.101471251,0.149936828,0.13737821,0.122858894,0.124237705,0.114269962,0.10661225,0.113119543,0.133753734,0.106586294,0.111790285,0.117011696,0.121608615,0.110669636,0.112977217,0.207469128,0.203602418,0.198745058,0.182456366,0.205799715,0.174582131,0.190026548,0.125836185,0.130621989,0.13953115,0.130320763,0.14078369,0.127375169,0.123354514,0.137876824,0.155466789,0.177494796,0.150001748,0.176117282,0.06978053,0.079354804,0.086266551,0.078232799,0.061032073,0.217371893,0.196512468,0.192091057,0.204160541,0.207611111,0.194708957,0.109859435,0.160428161,0.157931315,0.068432897,0.082929456,0.123617443,0.149811612,0.224884368,0.156521683,0.158232745,0.186290634,0.174714126,0.216954183,0.146484481,0.127483821,0.134983786,0.148051636,0.119505292,0.142189816,0.164576201,0.12276129,0.119109797,0.144839648,0.156252937,0.130409111,0.138118094,0.098845853,0.113236254,0.087506603,0.134124237,0.126118881,0.10164489,0.08896259,0.09026414,0.151856654,0.154371328,0.075286625,0.122685473,0.173466413,0.099856961,0.149086665,0.079312754,0.088751116,0.050849945,0.081300769,0.206741905,0.185099956,0.167679921,0.167095068,0.125106414,0.107847599,0.125440135,0.130660811,0.105050833,0.139695575,0.139769041,0.10186783,0.124587368,0.144413382,0.125683249,0.123190931,0.129270485,0.126442667,0.137105918,0.143115209,0.13987606,0.146102583,0.126892429,0.130310186,0.133237326,0.117311405,0.127334326,0.134640594,0.120555996,0.122867691,0.150556458,0.174932855,0.134372443,0.155805676,0.128707759,0.170925712,0.196988114,0.140342414,0.126459448,0.082099529,0.152395788,0.167399196,0.164478806,0.16140644,0.166261143,0.097555696,0.092599453,0.095342893,0.087218212,0.091045983,0.085570759,0.120936671,0.127678625,0.055481939,0.102123116,0.135683123,0.115429975,0.158271263,0.148361085,0.129500168,0.12228228,0.106734365,0.10085773,0.135020088,0.163556731,0.104232138,0.12925763,0.126489795,0.132754126,0.135753608,0.136219423,0.197920677,0.181166628,0.170100413,0.198505104,0.179999269,0.181711035,0.142399912,0.143500389,0.138368878,0.133054768,0.131471434,0.154548359,0.096588088,0.110295519,0.152070944,0.093506808,0.157910365,0.152843148,0.152592642,0.157803845,0.1540289,0.163234161,0.164717929,0.196798821,0.1738565,0.163018249,0.179060701,0.20017947,0.171138205,0.143255756,0.13949867,0.11273704,0.127854489,0.085560702,0.122264707,0.102576968,0.104332266,0.096112784,0.133479357,0.069399722,0.06572157,0.175185205,0.175611991,0.181281126,0.151706125,0.083383129,0.048101161,0.050596063,0.045975022])\n",
    "energy_max=np.array([-11.208,-18.547,-14.83,-10.516,-12.5,-17.119,-16.4,-17.655,-19.135,-17.814,-18.483,-18.038,-19.325,-18.514,-18.727,-18.227,-18.255,-5.6344,-7.9882,-6.9795,-6.8036,-6.6602,-17.798,-21.266,-18.452,-14.014,-16.339,-18.413,-14.837,-15.474,-22.777,-17.979,-20.688,-15.973,-16.314,-22.076,-17.7,-26.755,-18.701,-9.7517,-9.9687,-9.1842,-8.5776,-9.5226,-8.8492,-8.635,-23.977,-26.962,-25.326,-29.228,-19.844,-21.628,-18.628,-25.859,-26.52,-31.987,-28.134,-24.065,-28.983,-26.637,-22.581,-34.306,-22.065,-22.682,-21.561,-21.629,-22.797,-21.176,-21.359,-21.731,-20.035,-22.634,-24.275,-20.874,-19.063,-20.547,-19.878,-14.978,-17.513,-18.495,-16.962,-26.426,-15.458,-17.755,-17.737,-40.291,-13.895,-14.712,-14.27,-13.925,-18.908,-14.965,-22.965,-19.307,-19.285,-19.646,-18.683,-17.558,-18.264,-23.916,-24.178,-26.611,-23.528,-16.036,-15.709,-20.187,-21.008,-19.976,-21.744,-14.967,-14.414,-13.377,-16.635,-15.508,-16.935,-17.557,-26.573,-24.597,-21.946,-23.518,-26.081,-22.883,-16.865,-18.099,-17.734,-17.248,-18.192,-18.888,-15.93,-17.433,-14.496,-20.129,-17.896,-16.903,-17.16,-14.052,-20.001,-14.417,-12.332,-20.743,-17.395,-18.882,-15.615,-21.999,-24.13,-20.997,-17.459,-18.522,-16.455,-12.212,-24.999,-21.814,-23.062,-21.656,-23.051,-17.011,-21.823,-18.317,-21.961,-21.073,-14.77,-15.144,-15.735,-15.082,-7.7265,-10.285,-10.421,-8.9023,-11.87,-7.173,-6.1893,-9.9891,-6.2378,-12.857,-8.5443,-14.218,-6.4529,-8.7346,-8.4595,-5.8888,-12.62,-13.418,-12.659,-14.159,-10.831,-10.064,-11.243,-10.745,-11.875,-11.654,-11.301,-8.0795,-8.4027,-10.6,-8.6388,-17.34,-19.591,-17.68,-16.553,-17.583,-12.621,-17.39,-18.668,-16.049,-14.852,-12.046,-10.832,-10.495,-9.0872,-12.459,-11.972,-15.106,-14.245,-20.726,-13.028,-14.34,-14.158,-15.02,-11.979,-11.774,-21.263,-19.329,-23.175,-25.87,-23.426,-17.966,-15.747,-16.975,-20.382,-14.937,-15.616,-13.122,-11.387,-11.626,-12.587,-10.95,-10.485,-8.1992,-10,-11.287,-9.1056,-18.004,-15.411,-21.786,-19.07,-17.608,-18.047,-17.281,-16.786,-18.167,-16.718,-17.667,-17.057,-16.759,-28.802,-26.599,-19.263,-23.362,-28.275,-21.126,-12.5,-12.151,-13.167,-11.827,-18.397,-16.451,-16.908,-15.61,-18.928,-24.456,-22.871,-27.409,-24.319,-20.107,-27.523,-24.048,-26.433,-20.194,-18.782,-16.252])\n",
    "energy_slope_max=np.array([-0.00023906,-0.0010485,-0.00096011,-0.000249235,-0.00123325,0.000255485,-0.00084306,-0.00064263,-0.00174465,-0.00161535,-0.00079753,-0.00049242,-0.0011461,-0.00174265,-0.000739675,-0.0014926,-0.0010521,-0.00072141,-0.000548755,-0.000461685,-0.0010188,-0.00076539,-0.003864,-0.00139705,-0.00078591,-0.00109,-0.0012013,-0.0023102,-0.00117795,-0.0013958,0.000017132,-0.000286725,-0.00068579,-0.00019062,-0.00100905,-0.00117585,-0.00095941,-0.00204075,-0.00170465,-0.00056613,-0.0019903,-0.00076695,-0.0009804,-0.000726225,-0.00058993,-0.000517895,-0.001432,-0.00149725,-0.0013889,-0.00079545,-0.001398,-0.001343,-0.00116065,-0.000816985,-0.00095986,-0.001097795,-0.000617145,-5.02056E-05,-0.0010514,-0.000149605,-0.0003185,-0.001048,-0.0013354,-0.0028759,-0.00147305,-0.00168595,-0.00206,-0.00015837,-0.000904105,-0.00123775,-0.00040593,-0.00018011,-0.0011001,-0.0008004,-0.00061775,-0.0011425,-0.00043985,0.00014893,-0.0020323,-0.00049842,-0.0013289,-0.00097283,-0.00052939,-0.001082,-0.00067507,0.00061928,-0.00023691,-0.0007423,-0.0010235,-0.00073759,-0.000928245,-0.001000025,-0.000056575,-0.000605825,4.70795E-05,-0.00084854,-0.00073307,-0.00051498,-0.00076538,-0.00112,-0.00184585,-0.00134605,-0.00066146,-0.000753205,-0.0012344,-0.00020283,-0.00055849,-0.00067337,-0.000614695,-0.0011557,-0.000576925,-0.00047617,-0.00095116,-0.00052791,-0.00092257,-3.22475E-07,-5.73135E-06,-0.000037702,-0.000021273,-0.000019336,-4.3154E-06,-3.7812E-06,-0.00024645,-0.00077746,-0.00109845,-0.0028431,-0.0017232,-0.00143025,-0.0013881,-0.00131025,-0.00129055,-0.00228205,-0.001746,-0.0011588,-0.00150425,-0.0010743,-0.0006528,-0.0013272,-0.0010853,-0.000052854,-0.00129075,-0.0015457,-0.00095946,-0.000105675,-0.0016839,-0.000570375,-0.0014871,-0.00098162,-0.0007609,-0.00192975,-0.000220885,-0.0014863,-0.0024403,-0.001213,-0.0028386,-0.0014414,-0.0015378,-0.0014152,-0.0015017,-0.0014192,-0.00028111,-0.00052064,-0.00074356,-0.00069335,-0.0012527,-0.0025855,-0.0020994,-0.00094684,-0.00161895,-0.00099622,-0.0017598,-0.00094447,-0.0011068,-0.0025307,-0.00097978,-0.00297015,-0.001655,-0.0016076,-0.00188745,-0.0015515,-0.00117175,-0.0013592,-0.000785485,-0.0025872,-0.001455,-0.0012453,-0.00067105,-0.0012726,-0.00127425,-0.00162825,-0.00084351,-0.00097614,-0.000048319,-0.000130464,-0.00098546,-0.00072157,-0.00095007,-0.000535415,-0.001287,0.00024944,-0.00027088,-0.00067848,-0.00052253,-0.00047729,-0.000810265,-0.000714215,-0.0014547,-0.00120495,-0.00044116,-0.0014322,-0.0011869,-0.00037495,-0.0018881,-0.002088,-0.0024025,-0.00198495,-0.0013053,-0.00043451,-0.0014129,-0.00138075,-0.00082264,-0.0016834,-0.000869815,-0.00104835,-0.0011629,-0.0009097,-0.00070146,-0.00029846,-0.00079871,-0.00022916,-0.0004126,-0.0013814,-0.0016878,-0.00098295,-0.00110285,-0.0015368,-0.00136345,-0.00106665,-0.00120665,-0.00110835,-0.0017624,-0.00078145,-0.000993655,-0.00090133,-0.0010509,-0.000898705,-0.0024303,-0.0015732,-0.0011974,-0.0024291,-0.0011884,-0.00110795,-0.00091432,-0.0013367,-0.00030596,-0.0013265,-0.00151965,-0.00138875,-0.0016814,-0.00165515,-0.00111125,-0.0011542,-0.0014889,-0.000968205,-0.00079702,-0.00145045,-0.0019943,-0.0011395,-0.0016518,-0.001233,-0.00126725,-0.00049703,-0.00069208,-0.0010164,-0.00125275,-0.00066383,-0.00185585,-0.00089014,-0.0013696,-0.00054459])\n",
    "\n",
    "output_temp=np.array([0,0,0,1,1,0,0,0,-1,0,1,0,1,1,-1,0,1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,0,1,1,0,0,0,1,0,-1,-1,-1,1,-1,-1,0,0,-1,-1,1,1,1,1,1,-1,0,-1,0,0,0,1,0,1,1,1,0,-1,-1,-1,0,1,0,-1,-1,-1,0,-1,1,0,1,-1,1,0,0,0,0,0,0,-1,0,0,1,1,0,0,1,1,-1,1,-1,0,-1,-1,-1,-1,-1,0,-1,-1,-1,-1,-1,1,-1,1,1,1,1,1,1,1,1,1,-1,1,0,1,0,0,1,-1,1,0,-1,1,1,1,-1,0,-1,0,1,0,0,0,0,0,0,1,0,1,0,1,1,1,1,1,0,0,1,1,-1,-1,1,0,1,0,1,-1,-1,-1,-1,-1,0,-1,-1,-1,-1,-1,-1,0,-1,1,-1,1,-1,-1,-1,0,1,1,1,1,1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,0,1,0,-1,-1,1,-1,-1,-1,-1,0,-1,-1,-1,-1,-1,-1,0,-1,0,-1,-1,0,1,0,0,1,1,1,1,1,1,1,0,0,1,0,1,0,0,-1,0,1,0,0,0,0,1,1,-1,1,1,0,0,0,-1,1,-1,-1,-1,-1,1,0,0,0,1,0,1,1,0,-1,-1,0,-1,0,1,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(280, 3) (280,)\n"
     ]
    }
   ],
   "source": [
    "clf = SVC()\n",
    "feature_temp=np.array([naq_mean,energy_max,energy_slope_max])\n",
    "feature=np.transpose(feature_temp)\n",
    "#X=np.transpose(temp)[:-10]\n",
    "#Y=np.transpose(output)[:-10]\n",
    "#clf.set_params(kernel='linear').fit(X,Y)\n",
    "output=np.transpose(output_temp)\n",
    "print feature.shape,output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-93a50e5ec6a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "pred=clf.predict(X[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-5499ed30b210>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pred' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print accuracy_score(pred,Y[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "naq_mean=[0.233433266,0.271499846,0.194656705,0.181773902,0.241251601,0.265086984,0.14201744,0.150883289,0.09933675,0.177312719,0.145586967,0.157967267,0.102980098,0.158541599,0.133451922,0.169483455,0.162872673,0.065124556,0.040800376,0.095198734,0.111916183,0.094448743,0.167350417,0.14658334,0.077898064,0.133047281,0.130726566,0.129402373,0.190965075,0.134988408,0.210779379,0.284076145,0.282775033,0.279064789,0.088195386,0.075816688,0.085474097,0.089118429,0.09717783,0.196630779,0.136723469,0.173573746,0.143170418,0.155082261,0.197227479,0.18364364,0.17065009,0.150120044,0.158545951,0.190377017,0.095941138,0.088948628,0.095032433,0.184354567,0.146712652,0.158073212,0.166655461,0.198684544,0.204016179,0.190650649,0.172986713,0.162084907,0.097096912,0.111067612,0.107732716,0.113447461,0.108421046,0.148826254,0.123528583,0.14933202,0.101471251,0.149936828,0.13737821,0.122858894,0.124237705,0.114269962,0.10661225,0.113119543,0.133753734,0.106586294,0.111790285,0.117011696,0.121608615,0.110669636,0.112977217,0.207469128,0.203602418,0.198745058,0.182456366,0.205799715,0.174582131,0.190026548,0.125836185,0.130621989,0.13953115,0.130320763,0.14078369,0.127375169,0.123354514,0.137876824,0.155466789,0.177494796,0.150001748,0.176117282,0.06978053,0.079354804,0.086266551,0.078232799,0.061032073,0.217371893,0.196512468,0.192091057,0.204160541,0.207611111,0.194708957,0.109859435,0.160428161,0.157931315,0.068432897,0.082929456,0.123617443,0.149811612,0.224884368,0.156521683,0.158232745,0.186290634,0.174714126,0.216954183,0.146484481,0.127483821,0.134983786,0.148051636,0.119505292,0.142189816,0.164576201,0.12276129,0.119109797,0.144839648,0.156252937,0.130409111,0.138118094,0.098845853,0.113236254,0.087506603,0.134124237,0.126118881,0.10164489,0.08896259,0.09026414,0.151856654,0.154371328,0.075286625,0.122685473,0.173466413,0.099856961,0.149086665,0.079312754,0.088751116,0.050849945,0.081300769,0.206741905,0.185099956,0.167679921,0.167095068,0.125106414,0.107847599,0.125440135,0.130660811,0.105050833,0.139695575,0.139769041,0.10186783,0.124587368,0.144413382,0.125683249,0.123190931,0.129270485,0.126442667,0.137105918,0.143115209,0.13987606,0.146102583,0.126892429,0.130310186,0.133237326,0.117311405,0.127334326,0.134640594,0.120555996,0.122867691,0.150556458,0.174932855,0.134372443,0.155805676,0.128707759,0.170925712,0.196988114,0.140342414,0.126459448,0.082099529,0.152395788,0.167399196,0.164478806,0.16140644,0.166261143,0.097555696,0.092599453,0.095342893,0.087218212,0.091045983,0.085570759,0.120936671,0.127678625,0.055481939,0.102123116,0.135683123,0.115429975,0.158271263,0.148361085,0.129500168,0.12228228,0.106734365,0.10085773,0.135020088,0.163556731,0.104232138,0.12925763,0.126489795,0.132754126,0.135753608,0.136219423,0.197920677,0.181166628,0.170100413,0.198505104,0.179999269,0.181711035,0.142399912,0.143500389,0.138368878,0.133054768,0.131471434,0.154548359,0.096588088,0.110295519,0.152070944,0.093506808,0.157910365,0.152843148,0.152592642,0.157803845,0.1540289,0.163234161,0.164717929,0.196798821,0.1738565,0.163018249,0.179060701,0.20017947,0.171138205,0.143255756,0.13949867,0.11273704,0.127854489,0.085560702,0.122264707,0.102576968,0.104332266,0.096112784,0.133479357,0.069399722,0.06572157,0.175185205,0.175611991,0.181281126,0.151706125,0.083383129,0.048101161,0.050596063,0.045975022]\n",
    "energy_max=[-11.208,-18.547,-14.83,-10.516,-12.5,-17.119,-16.4,-17.655,-19.135,-17.814,-18.483,-18.038,-19.325,-18.514,-18.727,-18.227,-18.255,-5.6344,-7.9882,-6.9795,-6.8036,-6.6602,-17.798,-21.266,-18.452,-14.014,-16.339,-18.413,-14.837,-15.474,-22.777,-17.979,-20.688,-15.973,-16.314,-22.076,-17.7,-26.755,-18.701,-9.7517,-9.9687,-9.1842,-8.5776,-9.5226,-8.8492,-8.635,-23.977,-26.962,-25.326,-29.228,-19.844,-21.628,-18.628,-25.859,-26.52,-31.987,-28.134,-24.065,-28.983,-26.637,-22.581,-34.306,-22.065,-22.682,-21.561,-21.629,-22.797,-21.176,-21.359,-21.731,-20.035,-22.634,-24.275,-20.874,-19.063,-20.547,-19.878,-14.978,-17.513,-18.495,-16.962,-26.426,-15.458,-17.755,-17.737,-40.291,-13.895,-14.712,-14.27,-13.925,-18.908,-14.965,-22.965,-19.307,-19.285,-19.646,-18.683,-17.558,-18.264,-23.916,-24.178,-26.611,-23.528,-16.036,-15.709,-20.187,-21.008,-19.976,-21.744,-14.967,-14.414,-13.377,-16.635,-15.508,-16.935,-17.557,-26.573,-24.597,-21.946,-23.518,-26.081,-22.883,-16.865,-18.099,-17.734,-17.248,-18.192,-18.888,-15.93,-17.433,-14.496,-20.129,-17.896,-16.903,-17.16,-14.052,-20.001,-14.417,-12.332,-20.743,-17.395,-18.882,-15.615,-21.999,-24.13,-20.997,-17.459,-18.522,-16.455,-12.212,-24.999,-21.814,-23.062,-21.656,-23.051,-17.011,-21.823,-18.317,-21.961,-21.073,-14.77,-15.144,-15.735,-15.082,-7.7265,-10.285,-10.421,-8.9023,-11.87,-7.173,-6.1893,-9.9891,-6.2378,-12.857,-8.5443,-14.218,-6.4529,-8.7346,-8.4595,-5.8888,-12.62,-13.418,-12.659,-14.159,-10.831,-10.064,-11.243,-10.745,-11.875,-11.654,-11.301,-8.0795,-8.4027,-10.6,-8.6388,-17.34,-19.591,-17.68,-16.553,-17.583,-12.621,-17.39,-18.668,-16.049,-14.852,-12.046,-10.832,-10.495,-9.0872,-12.459,-11.972,-15.106,-14.245,-20.726,-13.028,-14.34,-14.158,-15.02,-11.979,-11.774,-21.263,-19.329,-23.175,-25.87,-23.426,-17.966,-15.747,-16.975,-20.382,-14.937,-15.616,-13.122,-11.387,-11.626,-12.587,-10.95,-10.485,-8.1992,-10,-11.287,-9.1056,-18.004,-15.411,-21.786,-19.07,-17.608,-18.047,-17.281,-16.786,-18.167,-16.718,-17.667,-17.057,-16.759,-28.802,-26.599,-19.263,-23.362,-28.275,-21.126,-12.5,-12.151,-13.167,-11.827,-18.397,-16.451,-16.908,-15.61,-18.928,-24.456,-22.871,-27.409,-24.319,-20.107,-27.523,-24.048,-26.433,-20.194,-18.782,-16.252]\n",
    "energy_slope_max=[-0.00023906,-0.0010485,-0.00096011,-0.000249235,-0.00123325,0.000255485,-0.00084306,-0.00064263,-0.00174465,-0.00161535,-0.00079753,-0.00049242,-0.0011461,-0.00174265,-0.000739675,-0.0014926,-0.0010521,-0.00072141,-0.000548755,-0.000461685,-0.0010188,-0.00076539,-0.003864,-0.00139705,-0.00078591,-0.00109,-0.0012013,-0.0023102,-0.00117795,-0.0013958,0.000017132,-0.000286725,-0.00068579,-0.00019062,-0.00100905,-0.00117585,-0.00095941,-0.00204075,-0.00170465,-0.00056613,-0.0019903,-0.00076695,-0.0009804,-0.000726225,-0.00058993,-0.000517895,-0.001432,-0.00149725,-0.0013889,-0.00079545,-0.001398,-0.001343,-0.00116065,-0.000816985,-0.00095986,-0.001097795,-0.000617145,-5.02056E-05,-0.0010514,-0.000149605,-0.0003185,-0.001048,-0.0013354,-0.0028759,-0.00147305,-0.00168595,-0.00206,-0.00015837,-0.000904105,-0.00123775,-0.00040593,-0.00018011,-0.0011001,-0.0008004,-0.00061775,-0.0011425,-0.00043985,0.00014893,-0.0020323,-0.00049842,-0.0013289,-0.00097283,-0.00052939,-0.001082,-0.00067507,0.00061928,-0.00023691,-0.0007423,-0.0010235,-0.00073759,-0.000928245,-0.001000025,-0.000056575,-0.000605825,4.70795E-05,-0.00084854,-0.00073307,-0.00051498,-0.00076538,-0.00112,-0.00184585,-0.00134605,-0.00066146,-0.000753205,-0.0012344,-0.00020283,-0.00055849,-0.00067337,-0.000614695,-0.0011557,-0.000576925,-0.00047617,-0.00095116,-0.00052791,-0.00092257,-3.22475E-07,-5.73135E-06,-0.000037702,-0.000021273,-0.000019336,-4.3154E-06,-3.7812E-06,-0.00024645,-0.00077746,-0.00109845,-0.0028431,-0.0017232,-0.00143025,-0.0013881,-0.00131025,-0.00129055,-0.00228205,-0.001746,-0.0011588,-0.00150425,-0.0010743,-0.0006528,-0.0013272,-0.0010853,-0.000052854,-0.00129075,-0.0015457,-0.00095946,-0.000105675,-0.0016839,-0.000570375,-0.0014871,-0.00098162,-0.0007609,-0.00192975,-0.000220885,-0.0014863,-0.0024403,-0.001213,-0.0028386,-0.0014414,-0.0015378,-0.0014152,-0.0015017,-0.0014192,-0.00028111,-0.00052064,-0.00074356,-0.00069335,-0.0012527,-0.0025855,-0.0020994,-0.00094684,-0.00161895,-0.00099622,-0.0017598,-0.00094447,-0.0011068,-0.0025307,-0.00097978,-0.00297015,-0.001655,-0.0016076,-0.00188745,-0.0015515,-0.00117175,-0.0013592,-0.000785485,-0.0025872,-0.001455,-0.0012453,-0.00067105,-0.0012726,-0.00127425,-0.00162825,-0.00084351,-0.00097614,-0.000048319,-0.000130464,-0.00098546,-0.00072157,-0.00095007,-0.000535415,-0.001287,0.00024944,-0.00027088,-0.00067848,-0.00052253,-0.00047729,-0.000810265,-0.000714215,-0.0014547,-0.00120495,-0.00044116,-0.0014322,-0.0011869,-0.00037495,-0.0018881,-0.002088,-0.0024025,-0.00198495,-0.0013053,-0.00043451,-0.0014129,-0.00138075,-0.00082264,-0.0016834,-0.000869815,-0.00104835,-0.0011629,-0.0009097,-0.00070146,-0.00029846,-0.00079871,-0.00022916,-0.0004126,-0.0013814,-0.0016878,-0.00098295,-0.00110285,-0.0015368,-0.00136345,-0.00106665,-0.00120665,-0.00110835,-0.0017624,-0.00078145,-0.000993655,-0.00090133,-0.0010509,-0.000898705,-0.0024303,-0.0015732,-0.0011974,-0.0024291,-0.0011884,-0.00110795,-0.00091432,-0.0013367,-0.00030596,-0.0013265,-0.00151965,-0.00138875,-0.0016814,-0.00165515,-0.00111125,-0.0011542,-0.0014889,-0.000968205,-0.00079702,-0.00145045,-0.0019943,-0.0011395,-0.0016518,-0.001233,-0.00126725,-0.00049703,-0.00069208,-0.0010164,-0.00125275,-0.00066383,-0.00185585,-0.00089014,-0.0013696,-0.00054459]\n",
    "gaze_up_down_min=[7,17,16,20,13,17,10,12,10,16,12,11,11,15,11,14,14,9,12,30,30,16,14,39,17,36,14,28,33,44,26,28,22,24,19,11,14,13,18,13,27,31,12,31,22,30,65,11,7,17,24,15,26,32,37,22,29,23,22,23,22,2,17,25,35,15,18,26,24,17,25,17,18,20,22,20,20,22,25,31,28,24,28,24,25,7,14,24,19,47,20,14,12,29,21,33,38,29,28,40,13,12,44,25,15,13,11,14,16,16,18,30,41,48,38,13,10,15,13,13,20,10,22,18,23,24,24,29,15,19,26,19,29,14,41,36,51,15,21,10,43,68,82,38,16,66,66,88,67,17,6,22,7,20,10,52,20,23,19,21,13,20,31,19,10,5,7,1,8,8,15,8,-1,12,6,7,6,10,13,11,23,29,58,33,17,22,20,18,24,19,20,23,23,20,23,24,22,23,22,21,7,12,6,7,13,13,15,30,19,17,19,19,38,11,13,16,27,15,13,16,25,20,21,18,27,12,23,21,50,16,15,30,28,22,29,28,26,8,10,14,16,86,29,29,38,39,22,47,50,39,43,41,57,23,43,18,14,23,19,27,35,26,18,26,26,21,19,24,32,-5,20,20,34,40,35,13,21,36,22,14]\n",
    "face_roll_max=[9,10,-7,-17,-17,-17,-5,-2,4,3,-11,-10,-9,-19,-11,-11,-3,0,0,-11,-2,-1,-3,-4,-20,-15,-5,-7,-12,-11,-2,-33,-22,-11,0,-2,-1,-1,-16,-8,-11,-12,-8,-9,-10,-18,-14,-22,0,-7,-3,-3,-7,-1,-4,-4,-2,-3,-5,-14,-3,2,-2,-1,-3,-2,1,-7,-4,-6,-6,-6,-8,-7,-13,-12,-7,0,-3,-9,-4,-1,0,-2,-2,9,2,-6,-5,-5,-4,-5,0,-3,0,3,0,-1,-1,-15,-15,-12,-15,-16,-7,-1,-2,-4,-3,-5,-6,-7,-3,-35,-6,-2,-2,-2,-2,-5,2,3,-3,0,-3,-3,-5,-8,-2,-6,-4,-1,-10,-8,-9,-6,-1,-5,-4,0,-3,-4,-5,-5,6,1,-6,-6,-5,-9,-4,-10,-25,-28,-6,-8,3,-8,-1,-8,3,-8,-4,-14,-11,-6,-12,-8,-15,-14,-16,-4,-24,-7,-3,-8,-19,-4,-13,-8,-3,-28,-11,-14,-5,-11,-5,-8,-6,-7,-2,2,-6,-5,-2,-2,-1,-1,-1,0,-13,-16,-6,-7,-19,4,2,-3,-2,6,0,4,3,6,4,4,3,4,0,3,-20,-3,-13,-14,-13,-2,-7,-4,-2,-11,-8,-8,-4,2,-13,-11,-11,7,4,-7,5,-9,-2,-12,-14,0,2,-2,1,-1,-5,-2,0,-1,-1,-6,-2,-3,-7,-17,-7,-18,-3,-17,2,2,2,1,0,1,-9,-3,-6,-53,-1,-11,-14,-8,-7,-12]\n",
    "mouth_open_mean=[26.94736842,60.26065574,56.01111111,69.10857143,60.82272727,42.10789474,27.74084507,48.01290323,53.45121951,78.45306122,62.06764706,43.475,35.49230769,0,0,0,0,27.58356164,39.03076923,57.56768707,61.084375,0,20.11111111,27.36290323,54.98120301,58.56408163,51.896,32,0,0,26.41818182,49.38082902,59.87777778,0,25.67047619,56.01906977,59.26136364,0,0,25.37325581,42.54677419,58.35,55.34015748,62.60380952,0,0,25.13690476,51.09386792,75.6525,43.27571429,39.80449438,48.52666667,62.39488189,26.94736842,57.3488189,43.8025,61.735,68.525,74.00263158,29.19189189,0,0,26.94736842,59.72753623,59.58110599,53.70212766,0,15.75,33.4826087,19.46410256,56.25731707,53.976,47.6047619,58.89480519,77.91875,43.98082192,61.48556701,29.05967742,42.49363636,60.01551724,53.87883212,0,0,0,0,0,31.68913043,37.77706422,53.73669725,62.67017544,73.76774194,63.44,38.76842105,24.76724138,47.0675,65.828125,58.64285714,58.35265957,55.40793651,25.3505618,56.57451737,61.1,63.47474747,23.24666667,42.94422111,47.27608696,51.54358974,60.37870968,60.67473684,22.72413793,43.59072848,55.12481203,69.940625,49.09358974,76.60877193,42.61403509,52.75454545,72.004,52.68636364,57.69752066,0,0,42.88215768,56.74705882,55.6578125,62.79519231,0,0,42.10061728,59.7419244,47.68507463,0,22.72413793,29.77959184,45.24588235,52.728125,65.371875,59.80973451,52.38815789,0,32.32237762,56.5625,57.34269663,68.63157895,16.63157895,60.675,0,0,0,24.77065217,46.41034483,59.01934426,56.35853659,68.54,0,0,42.34474708,58.56647059,60.60265487,0,28.09347826,46.24378378,61.2984,47.31625,29.394,24.42419355,60.9,50.05625,74.22073171,59.87614679,43.41311475,0,0,0,30.43559322,18.14545455,53.66335404,64.72557078,41.72033898,0,25.441,58.78382838,53.87883212,0,27.66111111,38.81518987,62.25762712,46.46190476,64.5460177,28.45,58.53193277,59.32608696,58.23876652,0,0,29.15271318,60.72110092,55.07386935,62.79519231,0,32.58979592,55.27518248,61.90740741,57.69752066,0,25.6979798,58.69197324,53.47394366,0,0,0,29.05967742,45.38559322,52.52941176,43.6877551,61.05,64.49530516,22.97692308,0,0,38.24624277,59.18869048,56.58291457,0,0,27.58356164,49.00277778,68.42545455,48.73571429,57.05862069,0,25.1816092,52.00754717,56.88387097,64.39005848,44.82857143,0,39.76222222,58.15460993,53.30128205,0,27.41756757,47.55631068,56.81858974,70.43492063,32.17638889,59.10212766,63.0555,54.73609023,25.63333333,0,0,0,0,26.78571429,58.61404255,62.05,45.4203125,0,0,28.75546875,60.81111111,56.15669291,28.8375,26.38655462,56.07843137,59.70261194,0,0,22.53333333,53.24644269,0,43.35787546,59.68533835,0,0,24.90487805,58.11213115,52.14771242,0]\n",
    "\n",
    "output_temp=[0,0,0,1,1,0,0,0,-1,0,1,0,1,1,-1,0,1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,0,1,1,0,0,0,1,0,-1,-1,-1,1,-1,-1,0,0,-1,-1,1,1,1,1,1,-1,0,-1,0,0,0,1,0,1,1,1,0,-1,-1,-1,0,1,0,-1,-1,-1,0,-1,1,0,1,-1,1,0,0,0,0,0,0,-1,0,0,1,1,0,0,1,1,-1,1,-1,0,-1,-1,-1,-1,-1,0,-1,-1,-1,-1,-1,1,-1,1,1,1,1,1,1,1,1,1,-1,1,0,1,0,0,1,-1,1,0,-1,1,1,1,-1,0,-1,0,1,0,0,0,0,0,0,1,0,1,0,1,1,1,1,1,0,0,1,1,-1,-1,1,0,1,0,1,-1,-1,-1,-1,-1,0,-1,-1,-1,-1,-1,-1,0,-1,1,-1,1,-1,-1,-1,0,1,1,1,1,1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,0,1,0,-1,-1,1,-1,-1,-1,-1,0,-1,-1,-1,-1,-1,-1,0,-1,0,-1,-1,0,1,0,0,1,1,1,1,1,1,1,0,0,1,0,1,0,0,-1,0,1,0,0,0,0,1,1,-1,1,1,0,0,0,-1,1,-1,-1,-1,-1,1,0,0,0,1,0,1,1,0,-1,-1,0,-1,0,1,0]\n",
    "\n",
    "output=np.transpose(output_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#p Value \n",
    "multi_p_1=[]\n",
    "multi_p_2=[]\n",
    "multi_p_3=[]\n",
    "multi_p_4=[]\n",
    "\n",
    "acoustic_p_1=[]\n",
    "acoustic_p_1=[]\n",
    "acoustic_p_1=[]\n",
    "acoustic_p_1=[]\n",
    "\n",
    "visual_p_1=[]\n",
    "visual_p_2=[]\n",
    "visual_p_3=[]\n",
    "visual_p_4=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Validation DataSet 0.357142857143 ***c=*** 0.1\n",
      "Accuracy for Validation DataSet 0.357142857143 ***c=*** 1.0\n",
      "Accuracy for Validation DataSet 0.328571428571 ***c=*** 10.0\n",
      "Accuracy for Validation DataSet 0.328571428571 ***c=*** 100.0\n",
      "Accuracy for Validation DataSet 0.271428571429 ***c=*** 1000.0\n",
      "Accuracy for Training DataSet: 0.510791366906 index= 0.1\n",
      "[-1  0 -1 -1 -1 -1 -1 -1  0  0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  0 -1\n",
      " -1 -1 -1 -1  1  0 -1 -1 -1 -1  0  0  1 -1 -1 -1 -1 -1 -1 -1 -1  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0 -1  1  0 -1]\n",
      "Accuracy for Test Dataset: 0.385714285714 index= 0.1\n",
      "\n",
      "\n",
      "Accuracy for Validation DataSet 0.385714285714 ***c=*** 0.1\n",
      "Accuracy for Validation DataSet 0.385714285714 ***c=*** 1.0\n",
      "Accuracy for Validation DataSet 0.414285714286 ***c=*** 10.0\n",
      "Accuracy for Validation DataSet 0.428571428571 ***c=*** 100.0\n",
      "Accuracy for Validation DataSet 0.457142857143 ***c=*** 1000.0\n",
      "Accuracy for Training DataSet: 0.503597122302 index= 1000.0\n",
      "[ 0  0  0  0  0  0  0 -1  0  0  0  1  1  1  1  0 -1 -1 -1 -1  0 -1  0 -1  0\n",
      "  0  0  0  0  1  0  0  0 -1 -1  0  0  0  0 -1 -1 -1  0 -1  0  0  0  0  0  0\n",
      "  0  1 -1  0  0  0  1  1 -1  0 -1  1 -1 -1 -1  0  0 -1 -1  1]\n",
      "Accuracy for Test Dataset: 0.271428571429 index= 1000.0\n",
      "\n",
      "\n",
      "Accuracy for Validation DataSet 0.3 ***c=*** 0.1\n",
      "Accuracy for Validation DataSet 0.3 ***c=*** 1.0\n",
      "Accuracy for Validation DataSet 0.328571428571 ***c=*** 10.0\n",
      "Accuracy for Validation DataSet 0.385714285714 ***c=*** 100.0\n",
      "Accuracy for Validation DataSet 0.328571428571 ***c=*** 1000.0\n",
      "Accuracy for Training DataSet: 0.467625899281 index= 100.0\n",
      "[ 0  0  0  0  0  0 -1  0 -1  1  0  0 -1 -1 -1 -1  0  0  0 -1  1  1  0  1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1  1 -1  0 -1 -1 -1  1 -1 -1 -1\n",
      "  1  1  1  1 -1  1  0  0  0 -1 -1  1  1  1 -1 -1 -1  0 -1 -1]\n",
      "Accuracy for Test Dataset: 0.442857142857 index= 100.0\n",
      "\n",
      "\n",
      "Accuracy for Validation DataSet 0.428571428571 ***c=*** 0.1\n",
      "Accuracy for Validation DataSet 0.428571428571 ***c=*** 1.0\n",
      "Accuracy for Validation DataSet 0.385714285714 ***c=*** 10.0\n",
      "Accuracy for Validation DataSet 0.457142857143 ***c=*** 100.0\n",
      "Accuracy for Validation DataSet 0.5 ***c=*** 1000.0\n",
      "Accuracy for Training DataSet: 0.489208633094 index= 1000.0\n",
      "[-1 -1  0 -1 -1 -1 -1  1 -1 -1 -1 -1 -1  1  1 -1 -1 -1  0 -1 -1  1  1  0  1\n",
      " -1  1 -1 -1 -1 -1  0  0 -1 -1  0 -1  0  0  0  1  1  0  1  0  1  1  1  1  1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1  1  0 -1  1  1 -1 -1 -1 -1]\n",
      "Accuracy for Test Dataset: 0.428571428571 index= 1000.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "kf = KFold(280, n_folds=4)\n",
    "clf_1=SVC()\n",
    "\n",
    "\n",
    "\n",
    "#C=[0.001,0.01,0.1,1,10,100,1000]\n",
    "C=[10E-2, 10E-1, 10E0, 10E1, 10E2]\n",
    "for train_index, test_index in kf:\n",
    "    X=[]\n",
    "    Y=[]\n",
    "    for i in train_index[71:]:#71 4 FOLD 93 - 3 fold\n",
    "        #print i\n",
    "        X.append([naq_mean[i],energy_max[i],energy_slope_max[i],gaze_up_down_min[i],face_roll_max[i],mouth_open_mean[i]])\n",
    "        Y.append(output[i])\n",
    "    \n",
    "    train_data=np.array(X)\n",
    "    #train_data=np.transpose(train_data)\n",
    "    output_data=np.array(Y)\n",
    "    train_output_data=np.transpose(output_data)\n",
    "    \n",
    "    validation_index=train_index[:70]#70 4 fold 92-3fold\n",
    "    X_val=[]\n",
    "    Y_val=[]\n",
    "    for i in validation_index:\n",
    "        X_val.append([naq_mean[i],energy_max[i],energy_slope_max[i],gaze_up_down_min[i],face_roll_max[i],mouth_open_mean[i]])\n",
    "        Y_val.append(output[i])\n",
    "    X_val=np.array(X_val)\n",
    "    #X_val=np.transpose(X_val)\n",
    "    Y_val=np.array(Y_val)\n",
    "    Y_val=np.transpose(Y_val)\n",
    "    accuracies=[]\n",
    "    max=0\n",
    "    index=0\n",
    "    for c in C:\n",
    "        clf_1.set_params(kernel='linear',C=c).fit(train_data,train_output_data)\n",
    "    \n",
    "        pred=clf_1.predict(X_val)\n",
    "        print \"Accuracy for Validation DataSet\", accuracy_score(pred,Y_val),\"***c=***\",c\n",
    "        \n",
    "        if max<np.asscalar(accuracy_score(pred,Y_val)):\n",
    "            max=np.asscalar(accuracy_score(pred,Y_val))\n",
    "            index=c\n",
    "\n",
    "    \n",
    "    clf_1.set_params(kernel='linear',C=index).fit(train_data,train_output_data)\n",
    "    \n",
    "    pred=clf_1.predict(train_data)\n",
    "\n",
    "    print \"Accuracy for Training DataSet:\",accuracy_score(pred,train_output_data), \"index=\",index\n",
    "\n",
    "    X_test=[]\n",
    "    Y_test=[]\n",
    "    for i in test_index:\n",
    "        #print i\n",
    "        X_test.append([naq_mean[i],energy_max[i],energy_slope_max[i],gaze_up_down_min[i],face_roll_max[i],mouth_open_mean[i]])\n",
    "        Y_test.append(output[i])\n",
    "    X_test=np.array(X_test)\n",
    "    Y_test=np.array(Y_test)\n",
    "    X_text=np.transpose(X_test)\n",
    "    Y_test=np.transpose(Y_test)\n",
    "    \n",
    "    pred=clf_1.predict(X_test)\n",
    "    print pred\n",
    "    print \"Accuracy for Test Dataset:\",accuracy_score(pred,Y_test), \"index=\",index\n",
    "    print \"\\n\"\n",
    "    \n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Validation DataSet 0.285714285714 ***c=*** 0.1\n",
      "Accuracy for Validation DataSet 0.285714285714 ***c=*** 1.0\n",
      "Accuracy for Validation DataSet 0.285714285714 ***c=*** 10.0\n",
      "Accuracy for Validation DataSet 0.257142857143 ***c=*** 100.0\n",
      "Accuracy for Validation DataSet 0.257142857143 ***c=*** 1000.0\n",
      "Accuracy for Training DataSet: 0.474820143885 index= 0.1\n",
      "[-1  0 -1 -1 -1 -1 -1 -1  0 -1  0  0  0  0  0  0  0 -1 -1 -1 -1 -1 -1  0  0\n",
      " -1 -1  0 -1 -1  0  0  0 -1 -1  0 -1  0  0 -1 -1 -1 -1 -1 -1 -1  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "Accuracy for Test Dataset: 0.4 index= 0.1\n",
      "\n",
      "\n",
      "Accuracy for Validation DataSet 0.4 ***c=*** 0.1\n",
      "Accuracy for Validation DataSet 0.385714285714 ***c=*** 1.0\n",
      "Accuracy for Validation DataSet 0.328571428571 ***c=*** 10.0\n",
      "Accuracy for Validation DataSet 0.328571428571 ***c=*** 100.0\n",
      "Accuracy for Validation DataSet 0.314285714286 ***c=*** 1000.0\n",
      "Accuracy for Training DataSet: 0.474820143885 index= 0.1\n",
      "[ 0  0  0  0  0  0  0 -1 -1  0 -1  0 -1 -1 -1  0 -1 -1 -1 -1  0 -1  0  0  0\n",
      "  0  0 -1  0  0  0  0  0 -1 -1  0  0  0  0 -1 -1 -1 -1 -1 -1 -1  0  0  0  0\n",
      "  0  0 -1  0 -1 -1  0  0 -1 -1 -1  0 -1 -1 -1 -1  0 -1 -1  0]\n",
      "Accuracy for Test Dataset: 0.285714285714 index= 0.1\n",
      "\n",
      "\n",
      "Accuracy for Validation DataSet 0.328571428571 ***c=*** 0.1\n",
      "Accuracy for Validation DataSet 0.314285714286 ***c=*** 1.0\n",
      "Accuracy for Validation DataSet 0.271428571429 ***c=*** 10.0\n",
      "Accuracy for Validation DataSet 0.285714285714 ***c=*** 100.0\n",
      "Accuracy for Validation DataSet 0.314285714286 ***c=*** 1000.0\n",
      "Accuracy for Training DataSet: 0.410071942446 index= 0.1\n",
      "[1 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1]\n",
      "Accuracy for Test Dataset: 0.3 index= 0.1\n",
      "\n",
      "\n",
      "Accuracy for Validation DataSet 0.357142857143 ***c=*** 0.1\n",
      "Accuracy for Validation DataSet 0.371428571429 ***c=*** 1.0\n",
      "Accuracy for Validation DataSet 0.428571428571 ***c=*** 10.0\n",
      "Accuracy for Validation DataSet 0.457142857143 ***c=*** 100.0\n",
      "Accuracy for Validation DataSet 0.471428571429 ***c=*** 1000.0\n",
      "Accuracy for Training DataSet: 0.467625899281 index= 1000.0\n",
      "[-1 -1 -1 -1 -1 -1 -1  1 -1 -1  1 -1 -1  1  1 -1 -1 -1  1 -1 -1  1  1 -1  1\n",
      "  1  1 -1 -1 -1 -1 -1  1 -1 -1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1  1  1  1  1  1 -1 -1 -1]\n",
      "Accuracy for Test Dataset: 0.342857142857 index= 1000.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Acoustic only\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "kf = KFold(280, n_folds=4)\n",
    "clf_1=SVC()\n",
    "\n",
    "#C=[0.001,0.01,0.1,1,10,100,1000]\n",
    "C=[10E-2, 10E-1, 10E0, 10E1, 10E2]\n",
    "for train_index, test_index in kf:\n",
    "    X=[]\n",
    "    Y=[]\n",
    "    for i in train_index[71:]:#71 4 FOLD\n",
    "        #print i\n",
    "        X.append([naq_mean[i],energy_max[i],energy_slope_max[i]])\n",
    "        Y.append(output[i])\n",
    "    \n",
    "    train_data=np.array(X)\n",
    "    #train_data=np.transpose(train_data)\n",
    "    output_data=np.array(Y)\n",
    "    train_output_data=np.transpose(output_data)\n",
    "    \n",
    "    validation_index=train_index[:70]#70 4 fold\n",
    "    X_val=[]\n",
    "    Y_val=[]\n",
    "    for i in validation_index:\n",
    "        X_val.append([naq_mean[i],energy_max[i],energy_slope_max[i]])\n",
    "        Y_val.append(output[i])\n",
    "    X_val=np.array(X_val)\n",
    "    #X_val=np.transpose(X_val)\n",
    "    Y_val=np.array(Y_val)\n",
    "    Y_val=np.transpose(Y_val)\n",
    "    accuracies=[]\n",
    "    max=0\n",
    "    index=0\n",
    "    for c in C:\n",
    "        clf_1.set_params(kernel='linear',C=c).fit(train_data,train_output_data)\n",
    "    \n",
    "        pred=clf_1.predict(X_val)\n",
    "        print \"Accuracy for Validation DataSet\", accuracy_score(pred,Y_val),\"***c=***\",c\n",
    "        \n",
    "        if max<np.asscalar(accuracy_score(pred,Y_val)):\n",
    "            max=np.asscalar(accuracy_score(pred,Y_val))\n",
    "            index=c\n",
    "\n",
    "    \n",
    "    clf_1.set_params(kernel='linear',C=index).fit(train_data,train_output_data)\n",
    "    \n",
    "    pred=clf_1.predict(train_data)\n",
    "\n",
    "    print \"Accuracy for Training DataSet:\",accuracy_score(pred,train_output_data), \"index=\",index\n",
    "\n",
    "    X_test=[]\n",
    "    Y_test=[]\n",
    "    for i in test_index:\n",
    "        #print i\n",
    "        X_test.append([naq_mean[i],energy_max[i],energy_slope_max[i]])\n",
    "        Y_test.append(output[i])\n",
    "    X_test=np.array(X_test)\n",
    "    Y_test=np.array(Y_test)\n",
    "    X_text=np.transpose(X_test)\n",
    "    Y_test=np.transpose(Y_test)\n",
    "    \n",
    "    pred=clf_1.predict(X_test)\n",
    "    print pred\n",
    "    print \"Accuracy for Test Dataset:\",accuracy_score(pred,Y_test), \"index=\",index\n",
    "    print \"\\n\"\n",
    "    \n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Validation DataSet 0.357142857143 ***c=*** 0.1\n",
      "Accuracy for Validation DataSet 0.357142857143 ***c=*** 1.0\n",
      "Accuracy for Validation DataSet 0.357142857143 ***c=*** 10.0\n",
      "Accuracy for Validation DataSet 0.357142857143 ***c=*** 100.0\n",
      "Accuracy for Validation DataSet 0.342857142857 ***c=*** 1000.0\n",
      "Accuracy for Training DataSet: 0.460431654676 index= 0.1\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1\n",
      " -1 -1 -1 -1  0 -1 -1 -1 -1 -1 -1 -1 -1 -1  0 -1 -1 -1 -1 -1]\n",
      "Accuracy for Test Dataset: 0.4 index= 0.1\n",
      "\n",
      "\n",
      "Accuracy for Validation DataSet 0.4 ***c=*** 0.1\n",
      "Accuracy for Validation DataSet 0.4 ***c=*** 1.0\n",
      "Accuracy for Validation DataSet 0.4 ***c=*** 10.0\n",
      "Accuracy for Validation DataSet 0.4 ***c=*** 100.0\n",
      "Accuracy for Validation DataSet 0.385714285714 ***c=*** 1000.0\n",
      "Accuracy for Training DataSet: 0.460431654676 index= 0.1\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  0 -1 -1 -1 -1 -1\n",
      "  0  0 -1 -1 -1 -1 -1  0 -1 -1 -1 -1 -1 -1 -1 -1 -1  0 -1  0 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  0 -1 -1 -1]\n",
      "Accuracy for Test Dataset: 0.357142857143 index= 0.1\n",
      "\n",
      "\n",
      "Accuracy for Validation DataSet 0.371428571429 ***c=*** 0.1\n",
      "Accuracy for Validation DataSet 0.371428571429 ***c=*** 1.0\n",
      "Accuracy for Validation DataSet 0.342857142857 ***c=*** 10.0\n",
      "Accuracy for Validation DataSet 0.357142857143 ***c=*** 100.0\n",
      "Accuracy for Validation DataSet 0.314285714286 ***c=*** 1000.0\n",
      "Accuracy for Training DataSet: 0.424460431655 index= 0.1\n",
      "[ 0  0  0  0  0  0  1  1  1 -1 -1  0 -1 -1 -1  1  0  0  0 -1  0  0  0 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  0 -1 -1  1 -1  0 -1 -1  1  0 -1  0 -1\n",
      "  0  0  0 -1 -1  0  0  0  0 -1 -1 -1  0  0 -1  0  0  0 -1 -1]\n",
      "Accuracy for Test Dataset: 0.485714285714 index= 0.1\n",
      "\n",
      "\n",
      "Accuracy for Validation DataSet 0.371428571429 ***c=*** 0.1\n",
      "Accuracy for Validation DataSet 0.371428571429 ***c=*** 1.0\n",
      "Accuracy for Validation DataSet 0.371428571429 ***c=*** 10.0\n",
      "Accuracy for Validation DataSet 0.371428571429 ***c=*** 100.0\n",
      "Accuracy for Validation DataSet 0.357142857143 ***c=*** 1000.0\n",
      "Accuracy for Training DataSet: 0.446043165468 index= 0.1\n",
      "[-1 -1  0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  0 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1  0 -1 -1  1 -1 -1  0  0 -1  1  1  0 -1  1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1]\n",
      "Accuracy for Test Dataset: 0.371428571429 index= 0.1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#VISUAL FEATURES\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "kf = KFold(280, n_folds=4)\n",
    "clf_1=SVC()\n",
    "\n",
    "#C=[0.001,0.01,0.1,1,10,100,1000]\n",
    "C=[10E-2, 10E-1, 10E0, 10E1, 10E2]\n",
    "for train_index, test_index in kf:\n",
    "    X=[]\n",
    "    Y=[]\n",
    "    for i in train_index[71:]:#71 4 FOLD\n",
    "        #print i\n",
    "        X.append([gaze_up_down_min[i],face_roll_max[i],mouth_open_mean[i]])\n",
    "        Y.append(output[i])\n",
    "    \n",
    "    train_data=np.array(X)\n",
    "    #train_data=np.transpose(train_data)\n",
    "    output_data=np.array(Y)\n",
    "    train_output_data=np.transpose(output_data)\n",
    "    \n",
    "    validation_index=train_index[:70]#70 4 fold\n",
    "    X_val=[]\n",
    "    Y_val=[]\n",
    "    for i in validation_index:\n",
    "        X_val.append([gaze_up_down_min[i],face_roll_max[i],mouth_open_mean[i]])\n",
    "        Y_val.append(output[i])\n",
    "    X_val=np.array(X_val)\n",
    "    #X_val=np.transpose(X_val)\n",
    "    Y_val=np.array(Y_val)\n",
    "    Y_val=np.transpose(Y_val)\n",
    "    accuracies=[]\n",
    "    max=0\n",
    "    index=0\n",
    "    for c in C:\n",
    "        clf_1.set_params(kernel='linear',C=c).fit(train_data,train_output_data)\n",
    "    \n",
    "        pred=clf_1.predict(X_val)\n",
    "        print \"Accuracy for Validation DataSet\", accuracy_score(pred,Y_val),\"***c=***\",c\n",
    "        \n",
    "        if max<np.asscalar(accuracy_score(pred,Y_val)):\n",
    "            max=np.asscalar(accuracy_score(pred,Y_val))\n",
    "            index=c\n",
    "\n",
    "    \n",
    "    clf_1.set_params(kernel='linear',C=index).fit(train_data,train_output_data)\n",
    "    \n",
    "    pred=clf_1.predict(train_data)\n",
    "\n",
    "    print \"Accuracy for Training DataSet:\",accuracy_score(pred,train_output_data), \"index=\",index\n",
    "\n",
    "    X_test=[]\n",
    "    Y_test=[]\n",
    "    for i in test_index:\n",
    "        #print i\n",
    "        X_test.append([gaze_up_down_min[i],face_roll_max[i],mouth_open_mean[i]])\n",
    "        Y_test.append(output[i])\n",
    "    X_test=np.array(X_test)\n",
    "    Y_test=np.array(Y_test)\n",
    "    X_text=np.transpose(X_test)\n",
    "    Y_test=np.transpose(Y_test)\n",
    "    \n",
    "    pred=clf_1.predict(X_test)\n",
    "    print pred\n",
    "    print \"Accuracy for Test Dataset:\",accuracy_score(pred,Y_test), \"index=\",index\n",
    "    print \"\\n\"\n",
    "    \n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Validation 0.314285714286 ***c=*** 0.1 ***gamma*** 0.1\n",
      "Accuracy for Validation 0.314285714286 ***c=*** 0.1 ***gamma*** 1\n",
      "Accuracy for Validation 0.314285714286 ***c=*** 0.1 ***gamma*** 10.0\n",
      "Accuracy for Validation 0.314285714286 ***c=*** 1.0 ***gamma*** 0.1\n",
      "Accuracy for Validation 0.314285714286 ***c=*** 1.0 ***gamma*** 1\n",
      "Accuracy for Validation 0.314285714286 ***c=*** 1.0 ***gamma*** 10.0\n",
      "Accuracy for Validation 0.342857142857 ***c=*** 10.0 ***gamma*** 0.1\n",
      "Accuracy for Validation 0.314285714286 ***c=*** 10.0 ***gamma*** 1\n",
      "Accuracy for Validation 0.314285714286 ***c=*** 10.0 ***gamma*** 10.0\n",
      "Accuracy for Validation 0.342857142857 ***c=*** 100.0 ***gamma*** 0.1\n",
      "Accuracy for Validation 0.314285714286 ***c=*** 100.0 ***gamma*** 1\n",
      "Accuracy for Validation 0.314285714286 ***c=*** 100.0 ***gamma*** 10.0\n",
      "Accuracy for Validation 0.342857142857 ***c=*** 1000.0 ***gamma*** 0.1\n",
      "Accuracy for Validation 0.314285714286 ***c=*** 1000.0 ***gamma*** 1\n",
      "Accuracy for Validation 0.314285714286 ***c=*** 1000.0 ***gamma*** 10.0\n",
      "\n",
      "\n",
      "Accuracy for Training DataSet: 1.0 index= 10.0 **gamma=*** 0.1\n",
      "Accuracy for test Dataset: 0.357142857143 index= 10.0\n",
      "\n",
      "\n",
      "Accuracy for Validation 0.371428571429 ***c=*** 0.1 ***gamma*** 0.1\n",
      "Accuracy for Validation 0.371428571429 ***c=*** 0.1 ***gamma*** 1\n",
      "Accuracy for Validation 0.371428571429 ***c=*** 0.1 ***gamma*** 10.0\n",
      "Accuracy for Validation 0.342857142857 ***c=*** 1.0 ***gamma*** 0.1\n",
      "Accuracy for Validation 0.371428571429 ***c=*** 1.0 ***gamma*** 1\n",
      "Accuracy for Validation 0.371428571429 ***c=*** 1.0 ***gamma*** 10.0\n",
      "Accuracy for Validation 0.357142857143 ***c=*** 10.0 ***gamma*** 0.1\n",
      "Accuracy for Validation 0.371428571429 ***c=*** 10.0 ***gamma*** 1\n",
      "Accuracy for Validation 0.371428571429 ***c=*** 10.0 ***gamma*** 10.0\n",
      "Accuracy for Validation 0.357142857143 ***c=*** 100.0 ***gamma*** 0.1\n",
      "Accuracy for Validation 0.371428571429 ***c=*** 100.0 ***gamma*** 1\n",
      "Accuracy for Validation 0.371428571429 ***c=*** 100.0 ***gamma*** 10.0\n",
      "Accuracy for Validation 0.357142857143 ***c=*** 1000.0 ***gamma*** 0.1\n",
      "Accuracy for Validation 0.371428571429 ***c=*** 1000.0 ***gamma*** 1\n",
      "Accuracy for Validation 0.371428571429 ***c=*** 1000.0 ***gamma*** 10.0\n",
      "\n",
      "\n",
      "Accuracy for Training DataSet: 0.402877697842 index= 0.1 **gamma=*** 0.1\n",
      "Accuracy for test Dataset: 0.314285714286 index= 0.1\n",
      "\n",
      "\n",
      "Accuracy for Validation 0.342857142857 ***c=*** 0.1 ***gamma*** 0.1\n",
      "Accuracy for Validation 0.342857142857 ***c=*** 0.1 ***gamma*** 1\n",
      "Accuracy for Validation 0.342857142857 ***c=*** 0.1 ***gamma*** 10.0\n",
      "Accuracy for Validation 0.3 ***c=*** 1.0 ***gamma*** 0.1\n",
      "Accuracy for Validation 0.342857142857 ***c=*** 1.0 ***gamma*** 1\n",
      "Accuracy for Validation 0.342857142857 ***c=*** 1.0 ***gamma*** 10.0\n",
      "Accuracy for Validation 0.3 ***c=*** 10.0 ***gamma*** 0.1\n",
      "Accuracy for Validation 0.342857142857 ***c=*** 10.0 ***gamma*** 1\n",
      "Accuracy for Validation 0.342857142857 ***c=*** 10.0 ***gamma*** 10.0\n",
      "Accuracy for Validation 0.3 ***c=*** 100.0 ***gamma*** 0.1\n",
      "Accuracy for Validation 0.342857142857 ***c=*** 100.0 ***gamma*** 1\n",
      "Accuracy for Validation 0.342857142857 ***c=*** 100.0 ***gamma*** 10.0\n",
      "Accuracy for Validation 0.3 ***c=*** 1000.0 ***gamma*** 0.1\n",
      "Accuracy for Validation 0.342857142857 ***c=*** 1000.0 ***gamma*** 1\n",
      "Accuracy for Validation 0.342857142857 ***c=*** 1000.0 ***gamma*** 10.0\n",
      "\n",
      "\n",
      "Accuracy for Training DataSet: 0.352517985612 index= 0.1 **gamma=*** 0.1\n",
      "Accuracy for test Dataset: 0.214285714286 index= 0.1\n",
      "\n",
      "\n",
      "Accuracy for Validation 0.371428571429 ***c=*** 0.1 ***gamma*** 0.1\n",
      "Accuracy for Validation 0.371428571429 ***c=*** 0.1 ***gamma*** 1\n",
      "Accuracy for Validation 0.371428571429 ***c=*** 0.1 ***gamma*** 10.0\n",
      "Accuracy for Validation 0.385714285714 ***c=*** 1.0 ***gamma*** 0.1\n",
      "Accuracy for Validation 0.371428571429 ***c=*** 1.0 ***gamma*** 1\n",
      "Accuracy for Validation 0.371428571429 ***c=*** 1.0 ***gamma*** 10.0\n",
      "Accuracy for Validation 0.371428571429 ***c=*** 10.0 ***gamma*** 0.1\n",
      "Accuracy for Validation 0.371428571429 ***c=*** 10.0 ***gamma*** 1\n",
      "Accuracy for Validation 0.371428571429 ***c=*** 10.0 ***gamma*** 10.0\n",
      "Accuracy for Validation 0.371428571429 ***c=*** 100.0 ***gamma*** 0.1\n",
      "Accuracy for Validation 0.371428571429 ***c=*** 100.0 ***gamma*** 1\n",
      "Accuracy for Validation 0.371428571429 ***c=*** 100.0 ***gamma*** 10.0\n",
      "Accuracy for Validation 0.371428571429 ***c=*** 1000.0 ***gamma*** 0.1\n",
      "Accuracy for Validation 0.371428571429 ***c=*** 1000.0 ***gamma*** 1\n",
      "Accuracy for Validation 0.371428571429 ***c=*** 1000.0 ***gamma*** 10.0\n",
      "\n",
      "\n",
      "Accuracy for Training DataSet: 0.992805755396 index= 1.0 **gamma=*** 0.1\n",
      "Accuracy for test Dataset: 0.328571428571 index= 1.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#RBF Kernel\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import KFold\n",
    "kf = KFold(280, n_folds=4)\n",
    "clf_rbf=SVC()\n",
    "\n",
    "C=[10E-2, 10E-1, 10E0, 10E1, 10E2]\n",
    "g=[1e-1, 1, 1e1]\n",
    "\n",
    "for train_index, test_index in kf:\n",
    "    X=[]\n",
    "    Y=[]\n",
    "    for i in train_index[71:]:\n",
    "        #print i\n",
    "        X.append([naq_mean[i],energy_max[i],energy_slope_max[i],gaze_up_down_min[i],face_roll_max[i],mouth_open_mean[i]])\n",
    "        Y.append(output[i])\n",
    "    \n",
    "    train_data=np.array(X)\n",
    "    #train_data=np.transpose(train_data)\n",
    "    output_data=np.array(Y)\n",
    "    train_output_data=np.transpose(output_data)\n",
    "    \n",
    "    validation_index=train_index[:70]\n",
    "    X_val=[]\n",
    "    Y_val=[]\n",
    "    for i in validation_index:\n",
    "        X_val.append([naq_mean[i],energy_max[i],energy_slope_max[i],gaze_up_down_min[i],face_roll_max[i],mouth_open_mean[i]])\n",
    "        Y_val.append(output[i])\n",
    "    X_val=np.array(X_val)\n",
    "    #X_val=np.transpose(X_val)\n",
    "    Y_val=np.array(Y_val)\n",
    "    Y_val=np.transpose(Y_val)\n",
    "    accuracies=[]\n",
    "    max=0\n",
    "    index=0\n",
    "    Gamma=0\n",
    "    for c in C:\n",
    "        for gamma in g:\n",
    "            clf_rbf.set_params(kernel='rbf',C=c,gamma=gamma).fit(train_data,train_output_data)\n",
    "    \n",
    "            pred=clf_rbf.predict(X_val)\n",
    "            print \"Accuracy for Validation\",accuracy_score(pred,Y_val),\"***c=***\",c, \"***gamma***\",gamma\n",
    "        \n",
    "            if max<np.asscalar(accuracy_score(pred,Y_val)):\n",
    "                max=np.asscalar(accuracy_score(pred,Y_val))\n",
    "                index=c\n",
    "                Gamma=gamma\n",
    "\n",
    "    \n",
    "    clf_rbf.set_params(kernel='rbf',C=index,gamma=Gamma).fit(train_data,train_output_data)\n",
    "\n",
    "    pred=clf_rbf.predict(train_data)\n",
    "    print \"\\n\"\n",
    "    print \"Accuracy for Training DataSet:\",accuracy_score(pred,train_output_data), \"index=\",index,\"**gamma=***\",Gamma\n",
    "    \n",
    "    X_test=[]\n",
    "    Y_test=[]\n",
    "    for i in test_index:\n",
    "        #print i\n",
    "        X_test.append([naq_mean[i],energy_max[i],energy_slope_max[i],gaze_up_down_min[i],face_roll_max[i],mouth_open_mean[i]])\n",
    "        Y_test.append(output[i])\n",
    "    X_test=np.array(X_test)\n",
    "    Y_test=np.array(Y_test)\n",
    "    X_text=np.transpose(X_test)\n",
    "    Y_test=np.transpose(Y_test)\n",
    "    \n",
    "    pred=clf_rbf.predict(X_test)\n",
    "\n",
    "    print \"Accuracy for test Dataset:\",accuracy_score(pred,Y_test), \"index=\",index\n",
    "    print \"\\n\"\n",
    "    \n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy for Validation 0.314285714286 ***c=*** 0.1 ***gamma*** 0.1\n",
      "Accuracy for Validation 0.314285714286 ***c=*** 0.1 ***gamma*** 1\n",
      "Accuracy for Validation 0.314285714286 ***c=*** 0.1 ***gamma*** 10.0\n",
      "Accuracy for Validation 0.257142857143 ***c=*** 1.0 ***gamma*** 0.1\n",
      "Accuracy for Validation 0.414285714286 ***c=*** 1.0 ***gamma*** 1\n",
      "Accuracy for Validation 0.328571428571 ***c=*** 1.0 ***gamma*** 10.0\n",
      "Accuracy for Validation 0.328571428571 ***c=*** 10.0 ***gamma*** 0.1\n",
      "Accuracy for Validation 0.4 ***c=*** 10.0 ***gamma*** 1\n",
      "Accuracy for Validation 0.328571428571 ***c=*** 10.0 ***gamma*** 10.0\n",
      "Accuracy for Validation 0.471428571429 ***c=*** 100.0 ***gamma*** 0.1\n",
      "Accuracy for Validation 0.4 ***c=*** 100.0 ***gamma*** 1\n",
      "Accuracy for Validation 0.328571428571 ***c=*** 100.0 ***gamma*** 10.0\n",
      "Accuracy for Validation 0.428571428571 ***c=*** 1000.0 ***gamma*** 0.1\n",
      "Accuracy for Validation 0.4 ***c=*** 1000.0 ***gamma*** 1\n",
      "Accuracy for Validation 0.328571428571 ***c=*** 1000.0 ***gamma*** 10.0\n",
      "\n",
      "\n",
      "Accuracy for Training DataSet: 0.935251798561 index= 100.0 **gamma=*** 0.1\n",
      "Accuracy for Validation 0.471428571429 ***c=*** 100.0 ***gamma*** 10.0\n",
      "Accuracy for test Dataset: 0.285714285714 index= 100.0\n",
      "\n",
      "\n",
      "Accuracy for Validation 0.371428571429 ***c=*** 0.1 ***gamma*** 0.1\n",
      "Accuracy for Validation 0.371428571429 ***c=*** 0.1 ***gamma*** 1\n",
      "Accuracy for Validation 0.371428571429 ***c=*** 0.1 ***gamma*** 10.0\n",
      "Accuracy for Validation 0.4 ***c=*** 1.0 ***gamma*** 0.1\n",
      "Accuracy for Validation 0.371428571429 ***c=*** 1.0 ***gamma*** 1\n",
      "Accuracy for Validation 0.371428571429 ***c=*** 1.0 ***gamma*** 10.0\n",
      "Accuracy for Validation 0.357142857143 ***c=*** 10.0 ***gamma*** 0.1\n",
      "Accuracy for Validation 0.4 ***c=*** 10.0 ***gamma*** 1\n",
      "Accuracy for Validation 0.371428571429 ***c=*** 10.0 ***gamma*** 10.0\n",
      "Accuracy for Validation 0.285714285714 ***c=*** 100.0 ***gamma*** 0.1\n",
      "Accuracy for Validation 0.4 ***c=*** 100.0 ***gamma*** 1\n",
      "Accuracy for Validation 0.371428571429 ***c=*** 100.0 ***gamma*** 10.0\n",
      "Accuracy for Validation 0.414285714286 ***c=*** 1000.0 ***gamma*** 0.1\n",
      "Accuracy for Validation 0.4 ***c=*** 1000.0 ***gamma*** 1\n",
      "Accuracy for Validation 0.371428571429 ***c=*** 1000.0 ***gamma*** 10.0\n",
      "\n",
      "\n",
      "Accuracy for Training DataSet: 1.0 index= 1000.0 **gamma=*** 0.1\n",
      "Accuracy for Validation 0.414285714286 ***c=*** 1000.0 ***gamma*** 10.0\n",
      "Accuracy for test Dataset: 0.428571428571 index= 1000.0\n",
      "\n",
      "\n",
      "Accuracy for Validation 0.342857142857 ***c=*** 0.1 ***gamma*** 0.1\n",
      "Accuracy for Validation 0.342857142857 ***c=*** 0.1 ***gamma*** 1\n",
      "Accuracy for Validation 0.342857142857 ***c=*** 0.1 ***gamma*** 10.0\n",
      "Accuracy for Validation 0.414285714286 ***c=*** 1.0 ***gamma*** 0.1\n",
      "Accuracy for Validation 0.414285714286 ***c=*** 1.0 ***gamma*** 1\n",
      "Accuracy for Validation 0.328571428571 ***c=*** 1.0 ***gamma*** 10.0\n",
      "Accuracy for Validation 0.4 ***c=*** 10.0 ***gamma*** 0.1\n",
      "Accuracy for Validation 0.457142857143 ***c=*** 10.0 ***gamma*** 1\n",
      "Accuracy for Validation 0.328571428571 ***c=*** 10.0 ***gamma*** 10.0\n",
      "Accuracy for Validation 0.371428571429 ***c=*** 100.0 ***gamma*** 0.1\n",
      "Accuracy for Validation 0.457142857143 ***c=*** 100.0 ***gamma*** 1\n",
      "Accuracy for Validation 0.328571428571 ***c=*** 100.0 ***gamma*** 10.0\n",
      "Accuracy for Validation 0.442857142857 ***c=*** 1000.0 ***gamma*** 0.1\n",
      "Accuracy for Validation 0.457142857143 ***c=*** 1000.0 ***gamma*** 1\n",
      "Accuracy for Validation 0.328571428571 ***c=*** 1000.0 ***gamma*** 10.0\n",
      "\n",
      "\n",
      "Accuracy for Training DataSet: 1.0 index= 10.0 **gamma=*** 1\n",
      "Accuracy for Validation 0.457142857143 ***c=*** 10.0 ***gamma*** 10.0\n",
      "Accuracy for test Dataset: 0.357142857143 index= 10.0\n",
      "\n",
      "\n",
      "Accuracy for Validation 0.371428571429 ***c=*** 0.1 ***gamma*** 0.1\n",
      "Accuracy for Validation 0.371428571429 ***c=*** 0.1 ***gamma*** 1\n",
      "Accuracy for Validation 0.371428571429 ***c=*** 0.1 ***gamma*** 10.0\n",
      "Accuracy for Validation 0.528571428571 ***c=*** 1.0 ***gamma*** 0.1\n",
      "Accuracy for Validation 0.371428571429 ***c=*** 1.0 ***gamma*** 1\n",
      "Accuracy for Validation 0.371428571429 ***c=*** 1.0 ***gamma*** 10.0\n",
      "Accuracy for Validation 0.4 ***c=*** 10.0 ***gamma*** 0.1\n",
      "Accuracy for Validation 0.342857142857 ***c=*** 10.0 ***gamma*** 1\n",
      "Accuracy for Validation 0.371428571429 ***c=*** 10.0 ***gamma*** 10.0\n",
      "Accuracy for Validation 0.342857142857 ***c=*** 100.0 ***gamma*** 0.1\n",
      "Accuracy for Validation 0.342857142857 ***c=*** 100.0 ***gamma*** 1\n",
      "Accuracy for Validation 0.371428571429 ***c=*** 100.0 ***gamma*** 10.0\n",
      "Accuracy for Validation 0.414285714286 ***c=*** 1000.0 ***gamma*** 0.1\n",
      "Accuracy for Validation 0.342857142857 ***c=*** 1000.0 ***gamma*** 1\n",
      "Accuracy for Validation 0.371428571429 ***c=*** 1000.0 ***gamma*** 10.0\n",
      "\n",
      "\n",
      "Accuracy for Training DataSet: 0.589928057554 index= 1.0 **gamma=*** 0.1\n",
      "Accuracy for Validation 0.528571428571 ***c=*** 1.0 ***gamma*** 10.0\n",
      "Accuracy for test Dataset: 0.428571428571 index= 1.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Normalized RBF:\n",
    "#RBF Kernel\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "scaler = StandardScaler() \n",
    "\n",
    "kf = KFold(280, n_folds=4)\n",
    "clf_rbf=SVC()\n",
    "\n",
    "C=[10E-2, 10E-1, 10E0, 10E1, 10E2]\n",
    "g=[1e-1, 1, 1e1]\n",
    "\n",
    "for train_index, test_index in kf:\n",
    "    X=[]\n",
    "    Y=[]\n",
    "    for i in train_index[71:]:\n",
    "        #print i\n",
    "        X.append([naq_mean[i],energy_max[i],energy_slope_max[i],gaze_up_down_min[i],face_roll_max[i],mouth_open_mean[i]])\n",
    "        Y.append(output[i])\n",
    "    \n",
    "    train_data=np.array(X)\n",
    "    #train_data=np.transpose(train_data)\n",
    "    output_data=np.array(Y)\n",
    "    train_output_data=np.transpose(output_data)\n",
    "    \n",
    "    scaler.fit(train_data) \n",
    "    X_train = scaler.transform(train_data)\n",
    "    \n",
    "    validation_index=train_index[:70]\n",
    "    X_val=[]\n",
    "    Y_val=[]\n",
    "    for i in validation_index:\n",
    "        X_val.append([naq_mean[i],energy_max[i],energy_slope_max[i],gaze_up_down_min[i],face_roll_max[i],mouth_open_mean[i]])\n",
    "        Y_val.append(output[i])\n",
    "    X_val=np.array(X_val)\n",
    "\n",
    "    X_val = scaler.transform(X_val)\n",
    "    #X_val=np.transpose(X_val)\n",
    "    Y_val=np.array(Y_val)\n",
    "    Y_val=np.transpose(Y_val)\n",
    "    accuracies=[]\n",
    "    max=0\n",
    "    index=0\n",
    "    Gamma=0\n",
    "    for c in C:\n",
    "        for gamma in g:\n",
    "            clf_rbf.set_params(kernel='rbf',C=c,gamma=gamma).fit(X_train,train_output_data)\n",
    "    \n",
    "            pred=clf_rbf.predict(X_val)\n",
    "            print \"Accuracy for Validation\",accuracy_score(pred,Y_val),\"***c=***\",c, \"***gamma***\",gamma\n",
    "        \n",
    "            if max<np.asscalar(accuracy_score(pred,Y_val)):\n",
    "                max=np.asscalar(accuracy_score(pred,Y_val))\n",
    "                index=c\n",
    "                Gamma=gamma\n",
    "\n",
    "    \n",
    "    clf_rbf.set_params(kernel='rbf',C=index,gamma=Gamma).fit(X_train,train_output_data)\n",
    "\n",
    "    pred=clf_rbf.predict(X_train)\n",
    "    print \"\\n\"\n",
    "    print \"Accuracy for Training DataSet:\",accuracy_score(pred,train_output_data), \"index=\",index,\"**gamma=***\",Gamma\n",
    "    pred=clf_rbf.predict(X_val)\n",
    "    print \"Accuracy for Validation\",accuracy_score(pred,Y_val),\"***c=***\",index, \"***gamma***\",gamma\n",
    "    \n",
    "    X_test=[]\n",
    "    Y_test=[]\n",
    "    for i in test_index:\n",
    "        #print i\n",
    "        X_test.append([naq_mean[i],energy_max[i],energy_slope_max[i],gaze_up_down_min[i],face_roll_max[i],mouth_open_mean[i]])\n",
    "        Y_test.append(output[i])\n",
    "    X_test=np.array(X_test)\n",
    "    Y_test=np.array(Y_test)\n",
    "\n",
    "    Y_test=np.transpose(Y_test)\n",
    "    \n",
    "    X_test = scaler.transform(X_test)\n",
    "    pred=clf_rbf.predict(X_test)\n",
    "\n",
    "    print \"Accuracy for test Dataset:\",accuracy_score(pred,Y_test), \"index=\",index\n",
    "    print \"\\n\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Validation DataSet 0.314285714286 ***c=*** 0.1\n",
      "Accuracy for Validation DataSet 0.314285714286 ***c=*** 1.0\n",
      "Accuracy for Validation DataSet 0.314285714286 ***c=*** 10.0\n",
      "Accuracy for Validation DataSet 0.3 ***c=*** 100.0\n",
      "Accuracy for Validation DataSet 0.3 ***c=*** 1000.0\n",
      "Accuracy for Training DataSet: 0.402877697842 index= 0.1\n",
      "Accuracy for Test Dataset: 0.371428571429 index= 0.1\n",
      "\n",
      "\n",
      "Accuracy for Validation DataSet 0.371428571429 ***c=*** 0.1\n",
      "Accuracy for Validation DataSet 0.371428571429 ***c=*** 1.0\n",
      "Accuracy for Validation DataSet 0.371428571429 ***c=*** 10.0\n",
      "Accuracy for Validation DataSet 0.414285714286 ***c=*** 100.0\n",
      "Accuracy for Validation DataSet 0.342857142857 ***c=*** 1000.0\n",
      "Accuracy for Training DataSet: 0.424460431655 index= 100.0\n",
      "Accuracy for Test Dataset: 0.3 index= 100.0\n",
      "\n",
      "\n",
      "Accuracy for Validation DataSet 0.342857142857 ***c=*** 0.1\n",
      "Accuracy for Validation DataSet 0.428571428571 ***c=*** 1.0\n",
      "Accuracy for Validation DataSet 0.4 ***c=*** 10.0\n",
      "Accuracy for Validation DataSet 0.385714285714 ***c=*** 100.0\n",
      "Accuracy for Validation DataSet 0.385714285714 ***c=*** 1000.0\n",
      "Accuracy for Training DataSet: 0.36690647482 index= 1.0\n",
      "Accuracy for Test Dataset: 0.285714285714 index= 1.0\n",
      "\n",
      "\n",
      "Accuracy for Validation DataSet 0.371428571429 ***c=*** 0.1\n",
      "Accuracy for Validation DataSet 0.371428571429 ***c=*** 1.0\n",
      "Accuracy for Validation DataSet 0.371428571429 ***c=*** 10.0\n",
      "Accuracy for Validation DataSet 0.271428571429 ***c=*** 100.0\n",
      "Accuracy for Validation DataSet 0.271428571429 ***c=*** 1000.0\n",
      "Accuracy for Training DataSet: 0.395683453237 index= 0.1\n",
      "Accuracy for Test Dataset: 0.314285714286 index= 0.1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Running Normalized Data\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "kf = KFold(280, n_folds=4)\n",
    "clf_norm=SVC()\n",
    "\n",
    "#C=[0.001,0.01,0.1,1,10,100,1000]\n",
    "C=[10E-2, 10E-1, 10E0, 10E1, 10E2]\n",
    "for train_index, test_index in kf:\n",
    "    X=[]\n",
    "    Y=[]\n",
    "    for i in train_index[71:]:#71 4 FOLD,93 3 fold\n",
    "        #print i\n",
    "        X.append([naq_mean[i],energy_max[i],energy_slope_max[i],gaze_up_down_min[i],face_roll_max[i],mouth_open_mean[i]])\n",
    "        Y.append(output[i])\n",
    "    \n",
    "    train_data=np.array(X)\n",
    "    #train_data=np.transpose(train_data)\n",
    "    output_data=np.array(Y)\n",
    "    train_output_data=np.transpose(output_data)\n",
    "\n",
    "    #Scaling\n",
    "    train_data_norm = preprocessing.normalize(train_data, norm='l2')\n",
    "    \n",
    "    \n",
    "    validation_index=train_index[:70]#70 4 fold, 92 3 fold\n",
    "    X_val=[]\n",
    "    Y_val=[]\n",
    "    for i in validation_index:\n",
    "        X_val.append([naq_mean[i],energy_max[i],energy_slope_max[i],gaze_up_down_min[i],face_roll_max[i],mouth_open_mean[i]])\n",
    "        Y_val.append(output[i])\n",
    "    X_val=np.array(X_val)\n",
    "    \n",
    "    X_val_norm = preprocessing.normalize(X_val, norm='l2')\n",
    "    #X_val=np.transpose(X_val)\n",
    "    Y_val=np.array(Y_val)\n",
    "    Y_val=np.transpose(Y_val)\n",
    "    accuracies=[]\n",
    "    max=0\n",
    "    index=0\n",
    "    for c in C:\n",
    "        clf_norm.set_params(kernel='linear',C=c).fit(train_data_norm,train_output_data)\n",
    "    \n",
    "        pred=clf_norm.predict(X_val_norm)\n",
    "        print \"Accuracy for Validation DataSet\", accuracy_score(pred,Y_val),\"***c=***\",c\n",
    "        \n",
    "        if max<np.asscalar(accuracy_score(pred,Y_val)):\n",
    "            max=np.asscalar(accuracy_score(pred,Y_val))\n",
    "            index=c\n",
    "\n",
    "    clf_norm.set_params(kernel='linear',C=index).fit(train_data_norm,train_output_data)\n",
    "    \n",
    "    pred=clf_norm.predict(train_data_norm)\n",
    "\n",
    "    print \"Accuracy for Training DataSet:\",accuracy_score(pred,train_output_data), \"index=\",index\n",
    "\n",
    "    X_test=[]\n",
    "    Y_test=[]\n",
    "    for i in test_index:\n",
    "        #print i\n",
    "        X_test.append([naq_mean[i],energy_max[i],energy_slope_max[i],gaze_up_down_min[i],face_roll_max[i],mouth_open_mean[i]])\n",
    "        Y_test.append(output[i])\n",
    "    X_test=np.array(X_test)\n",
    "    Y_test=np.array(Y_test)\n",
    "    X_text=np.transpose(X_test)\n",
    "    X_test_norm = preprocessing.normalize(X_test, norm='l2')\n",
    "    Y_test=np.transpose(Y_test)\n",
    "    \n",
    "    pred=clf_norm.predict(X_test_norm)\n",
    "\n",
    "    print \"Accuracy for Test Dataset:\",accuracy_score(pred,Y_test), \"index=\",index\n",
    "    print \"\\n\"\n",
    "    \n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Accuracy for Training DataSet: 0.47619047619\n",
      "Accuracy for test Dataset: 0.414285714286\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Accuracy for Training DataSet: 0.547619047619\n",
      "Accuracy for test Dataset: 0.357142857143\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Accuracy for Training DataSet: 0.528571428571\n",
      "Accuracy for test Dataset: 0.557142857143\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Accuracy for Training DataSet: 0.52380952381\n",
      "Accuracy for test Dataset: 0.4\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Naive Bayes Classifier\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.cross_validation import KFold\n",
    "kf = KFold(280, n_folds=4)\n",
    "clf_nb = GaussianNB()\n",
    "#clf.fit(features_train, labels_train)\n",
    "\n",
    "\n",
    "for train_index, test_index in kf:\n",
    "    X=[]\n",
    "    Y=[]\n",
    "    for i in train_index:\n",
    "        #print i\n",
    "        X.append([naq_mean[i],energy_max[i],energy_slope_max[i],gaze_up_down_min[i],face_roll_max[i],mouth_open_mean[i]])\n",
    "        Y.append(output[i])\n",
    "    \n",
    "    train_data=np.array(X)\n",
    "    #train_data=np.transpose(train_data)\n",
    "    output_data=np.array(Y)\n",
    "    train_output_data=np.transpose(output_data)\n",
    "    \n",
    "    validation_index=train_index[:70]\n",
    "    X_val=[]\n",
    "    Y_val=[]\n",
    "    for i in validation_index:\n",
    "        X_val.append([naq_mean[i],energy_max[i],energy_slope_max[i],gaze_up_down_min[i],face_roll_max[i],mouth_open_mean[i]])\n",
    "        Y_val.append(output[i])\n",
    "    X_val=np.array(X_val)\n",
    "    #X_val=np.transpose(X_val)\n",
    "    Y_val=np.array(Y_val)\n",
    "    Y_val=np.transpose(Y_val)\n",
    "    accuracies=[]\n",
    "    max=0\n",
    "    index=0\n",
    "    Gamma=0\n",
    "\n",
    "    clf_nb.fit(train_data,train_output_data)\n",
    "    pred=clf_nb.predict(train_data)\n",
    "    \n",
    "    print \"\\n\"\n",
    "    print \"Accuracy for Training DataSet:\",accuracy_score(pred,train_output_data)\n",
    "    \n",
    "    X_test=[]\n",
    "    Y_test=[]\n",
    "    for i in test_index:\n",
    "        #print i\n",
    "        X_test.append([naq_mean[i],energy_max[i],energy_slope_max[i],gaze_up_down_min[i],face_roll_max[i],mouth_open_mean[i]])\n",
    "        Y_test.append(output[i])\n",
    "    X_test=np.array(X_test)\n",
    "    Y_test=np.array(Y_test)\n",
    "    X_text=np.transpose(X_test)\n",
    "    Y_test=np.transpose(Y_test)\n",
    "    \n",
    "    pred=clf_nb.predict(X_test)\n",
    "\n",
    "    print \"Accuracy for test Dataset:\",accuracy_score(pred,Y_test)\n",
    "    print \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "#ANOVA CALCULATIONS:\n",
    "f_anova_path=\"/Users/apple/Desktop/USC/*Sem 3/CSCI 535/Homework/Homework_1/anova_input\"\n",
    "f_anova=open(f_anova_path,\"r+\")\n",
    "data=f_anova.read().strip()\n",
    "data_list=data.split('\\n')\n",
    "#print data_list\n",
    "clean_list=[]\n",
    "for i in data_list:\n",
    "    clean_list.append(i.split('\\t'))\n",
    "\n",
    "    \n",
    "zero_mean=[]\n",
    "positive_mean=[]\n",
    "negative_mean=[]\n",
    "\n",
    "zero_median=[]\n",
    "positive_median=[]\n",
    "negative_median=[]\n",
    "\n",
    "zero_min=[]\n",
    "positive_min=[]\n",
    "negative_min=[]\n",
    "\n",
    "zero_max=[]\n",
    "positive_max=[]\n",
    "negative_max=[]\n",
    "\n",
    "zero_sd=[]\n",
    "positive_sd=[]\n",
    "negative_sd=[]\n",
    "\n",
    "for i in clean_list:\n",
    "    if i[0]=='0':\n",
    "        zero_mean.append(float(i[1]))\n",
    "        zero_median.append(float(i[2]))\n",
    "        zero_min.append(float(i[3]))\n",
    "        zero_max.append(float(i[4]))\n",
    "        zero_sd.append(float(i[5]))\n",
    "    elif i[0]=='1':\n",
    "        positive_mean.append(float(i[1]))\n",
    "        positive_median.append(float(i[2]))\n",
    "        positive_min.append(float(i[3]))\n",
    "        positive_max.append(float(i[4]))\n",
    "        positive_sd.append(float(i[5]))\n",
    "    elif i[0]=='-1':\n",
    "        negative_mean.append(float(i[1]))\n",
    "        negative_median.append(float(i[2]))\n",
    "        negative_min.append(float(i[3]))\n",
    "        negative_max.append(float(i[4]))\n",
    "        negative_sd.append(float(i[5]))\n",
    "\n",
    "lol1=[-0.365391459,-0.369030548,-0.364743949,-0.379451112,-0.305015419,-0.297578286,-0.266549185,-0.30315411,-0.299457503,-0.303346151,-0.308257727,-0.347098208,-0.355732531,-0.180896173,-0.267892058,-0.241806191,-0.183570507,-0.278683314,-0.240962024,-0.273951447,-0.260498589,-0.283200698,-0.259957269,-0.243882646,-0.269418184,-0.233689957,-0.314321154,-0.31290612,-0.30196967,-0.329568122,-0.336472205,-0.318939791,-0.36193359,-0.306533712,-0.334442347,-0.313822929,-0.271833542,-0.255032779,-0.269964631,-0.294937715,-0.298544292,-0.276538545,-0.214390866,-0.196505126,-0.224271042,-0.186519212,-0.3311463,-0.293817329,-0.298858146,-0.272253322,-0.299306335,-0.26810536,-0.351134247,-0.311962135,-0.29705214,-0.294383957,-0.202104361,-0.193169393,-0.213504615,-0.309280026,-0.267262097,-0.225838114,-0.285011039,-0.291229608,-0.255098612,-0.23672206,-0.261753705,-0.214075088,-0.320411363,-0.338960672,-0.170496614,-0.217918958,-0.215175627,-0.079170497,-0.149165295,-0.101035804,-0.11196472,-0.31357172,-0.310843658,-0.294854584,-0.335673428,-0.30212085,-0.307908665,-0.276634383,-0.301444755,-0.307806948,-0.12509948,-0.10358111]\n",
    "lol2=[-0.337059743,-0.358565517,-0.298927737,-0.248434377,-0.30475846,-0.319758175,-0.369000473,-0.298937479,-0.315495325,-0.199583117,-0.227961743,-0.252334154,-0.249711003,-0.257615316,-0.304388828,-0.276469039,-0.275431789,-0.265935755,-0.263204002,-0.26706734,-0.232194714,-0.224621975,-0.328583042,-0.328782075,-0.324997524,-0.328743758,-0.27224877,-0.263546577,-0.284137079,-0.339286317,-0.339199932,-0.360938558,-0.345837129,-0.33919568,-0.261595719,-0.30741783,-0.239779773,-0.199129157,-0.285031141,-0.379509402,-0.323523418,-0.350984058,-0.259205972,-0.287996215,-0.198288,-0.208685481,-0.354985952,-0.268497692,-0.282087529,-0.302111832,-0.360721979,-0.298787005,-0.347669927,-0.304886748,-0.073389744,-0.101047948,-0.272258947,-0.282242277,-0.213986884,-0.236037267,-0.201660064,-0.209203019,-0.221387309,-0.215232999,-0.297294012,-0.312963745,-0.23585922,-0.245884531,-0.252369046,-0.239642468,-0.265990106,-0.251664802,-0.264196881,-0.235572725,-0.205377827,-0.315822724,-0.316334062,-0.109595271,-0.141271006,-0.14982667,-0.160768759,-0.331971471,-0.279926556,-0.232135742,-0.285201345,-0.237538996,-0.238391421,-0.137831708]\n",
    "lol3=[-0.341694063,-0.308899409,-0.264485208,-0.246134496,-0.265575694,-0.243236504,-0.240114941,-0.332321771,-0.307220199,-0.288245955,-0.314356923,-0.319828719,-0.189568681,-0.188036442,-0.217253867,-0.321984976,-0.294828867,-0.273872556,-0.260510296,-0.218873744,-0.201506964,-0.207599494,-0.256044192,-0.250545566,-0.265349623,-0.26060215,-0.242157754,-0.233597923,-0.218432278,-0.308601742,-0.253168148,-0.265745121,-0.262170821,-0.255517236,-0.261585192,-0.2738627,-0.259741811,-0.294176471,-0.272587776,-0.301875541,-0.273184819,-0.27891995,-0.36677761,-0.239744939,-0.319551976,-0.260539418,-0.196497903,-0.195175462,-0.097287578,-0.088446209,-0.206222345,-0.202160325,-0.205187865,-0.204134295,-0.214113018,-0.180023318,-0.188259708,-0.218331652,-0.203712612,-0.185723734,-0.183566308,-0.222999612,-0.205924857,-0.210055537,-0.266341709,-0.25743777,-0.318418499,-0.350603304,-0.323816315,-0.327575778,-0.302771628,-0.291686208,-0.288795074,-0.26513005,-0.267118678,-0.253245752,-0.29753231,-0.321719119,-0.320986336,-0.259660041,-0.268604396,-0.232526883,-0.266306337,-0.295099583,-0.279653817,-0.27124303,-0.289818957,-0.28828174,-0.292116197,-0.269085355,-0.291140923,-0.283289928,-0.290593568,-0.301653372,-0.222726143,-0.111826011,-0.329497423,-0.250291993,-0.250112845,-0.216817759,-0.245502446,-0.309408666,-0.300649668,-0.201176861]\n",
    "#print stats.f_oneway(lol1,lol2,lol3)\n",
    "print stats.f_oneway(zero_mean,positive_mean,negative_mean)\n",
    "print stats.f_oneway(zero_median,positive_median,negative_median)\n",
    "print stats.f_oneway(zero_min,positive_min,negative_min)\n",
    "print stats.f_oneway(zero_max,positive_max,negative_max)\n",
    "print stats.f_oneway(zero_sd,positive_sd,negative_sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn import preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df=pd.read_excel(\"/Users/apple/Desktop/USC/*Sem 3/Final_Feature_List.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "columns = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "minmax_scale=preprocessing.Normalizer().fit(df[columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_minmax=minmax_scale.transform(df[columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ndf=pd.DataFrame(df_minmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer=pd.ExcelWriter(\"/Users/apple/Desktop/USC/*Sem 3/ndf.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ndf.to_excel(writer,'Sheet 1')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
